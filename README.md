## Machine Learning & Deep Learning

Loose path:
1. Math
2. Programming
3. Machine Learning concepts
4. Specializations


### Math
Understanding Math is pivotal. You can never be a good Machine Learning Scientist
by skipping the Math.

  1. [Probability & Statistics](https://www.khanacademy.org/math/probability)
     Basic Probability and Stats will be helpful in understanding ML algorithms like Naive Bayes. 
     
  2. [Statistics 101 - Udacity](https://www.udacity.com/course/intro-to-statistics--st101)
      Taught by the founder of GoogleX it's full of exercises in Python so you won't get bored.
     
  3. [MIT 18.06 Linear Algebra](https://www.youtube.com/watch?v=ZK3O402wf1c&list=PLE7DDD91010BC51F8)
     Prof. Strang is terrific! Not only he'll make you fall in love in Linear Algebra but you'll learn
     important concepts like SVD and matrix algebra. You might wanna grab this [PDF](http://www.math.hcmus.edu.vn/~bxthang/Linear%20algebra%20and%20its%20applications.pdf)
     as well. Be sure to also solve the exam question papers from here: [link](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/exams/)
     
  4. [MIT Single Variable Calculus](https://www.youtube.com/watch?v=7K1sB05pE0A&list=PL590CCC2BC5AF3BC1)
     This is my personal favorite book, use it for SVC + MVC [link](https://drive.google.com/open?id=0BwEXorNDIEnFc3VKN3RUOWdRdUE)
     Amazing course but it gets quite tedious in the middle, you might wanna skim some geometry, but the key is
     to understand how optimization works. Be sure to solve questions from here: [link](https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/exams/)
     
  5. [MIT Multi Variable Calculus](https://www.youtube.com/watch?v=PxCxlsl_YwY&list=PL4C4C8A7D06566F38)
     Understanding vector calculus is necessary for algorithms like SVM, you might wanna skim some parts
     which are purely theoretical. Be sure to solve questions from here: [link](https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/exams/)
     
  7. (Optional) [Stanford Convex Optimization](https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about)
     WARNING: Do this course only if you're very good at math. Convex Optimization will teach you numerous
     functions used in Machine Learning. But this course is extremely heavy on Math!

### Introduction to Programming & Algorithms
  * Python - Any one, both courses are equally good
    1. [Complete Python BootCamp](https://www.udemy.com/complete-python-bootcamp/)
    2. [Complete Python Masterclass](https://www.udemy.com/python-the-complete-python-developer-course/)
    
  * Algorithms
  
    Since you'll be coding a lot of algorithms yourself basic understanding is necessary
    1. [Basic Algorithms and Complexity Theory](https://www.youtube.com/watch?v=o4SGkB_8fFs&list=PLhQjrBD2T382VRUw5ZpSxQSFrxMOdFObl)
    
    In case you want to go deeper
      1. [Algorithms Stanford I](http://online.stanford.edu/course/algorithms-design-and-analysis-part-1)
      2. [Algotithms Stanford II](http://online.stanford.edu/course/algorithms-design-and-analysis-part-2)
 
 
### Introduction to Machine Learning

  1. [Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning)
     A must do course, best course of Introduction to Machine Learning so far, light on Math and focuses more on concepts.
     
  Complete one out of two:
  
  1. [Machine Learning A-Z](https://www.udemy.com/machinelearning/)
     Introductory course on ML focusing on not only Python but also R, one of the best sellers on Udemy.

  2. [Introduction to Machine Learning - Udacity](https://www.udacity.com/course/intro-to-machine-learning--ud120)
     Sebastian Thrun does an awesome job explaining various approaches in ML. It gets a little boring in the middle
     but overall it's very good. 


### Applied Machine Learning
  Two quick courses on applying the theory you learnt. They're short so I recommend doing both of them. 
  
  1. [Python for Data Science and Machine Learning Bootcamp](https://www.udemy.com/python-for-data-science-and-machine-learning-bootcamp/)

  2. [Machine Learning with Python - Hands On!](https://www.udemy.com/data-science-and-machine-learning-with-python-hands-on/)
  
    
### Specializations

  * Deep Learning
  
    1. [Neural Networks by Geofrrey Hinton](https://www.coursera.org/learn/neural-networks)
       This guy is the creator of backpropagation algorithm! Warning: very heavy on Math.
       
    2. [MIT Introduction to Deep Learning](http://introtodeeplearning.com/index.html)
    
    2. [Deep Learning A-Z](https://www.udemy.com/deeplearning/)
    
    3. [Creative Applications of Deep Learning with TensorFlow](https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow/info)
   
    4. Must read book on Deep Learning: [Free HTML book](http://www.deeplearningbook.org/)

  * Big Data & Large Scale Machine Learning
  
    1. [Introduction to Big Data](https://www.coursera.org/learn/big-data-introduction)
    2. [Spark and Python for Big Data with PySpark](https://www.udemy.com/spark-and-python-for-big-data-with-pyspark/)
    3. [Distributed Machine Learning with Apache Spark](https://www.edx.org/course/distributed-machine-learning-apache-uc-berkeleyx-cs120x)
    
  * Natural Language Processing
  
    1. [Introduction to Natural Language Processing UMichigan](http://academictorrents.com/details/78515f90de063ffc144be5e7e726c03849b4e0ed)
    2. [Natural Language Processing by Stanford](http://academictorrents.com/details/d2c8f8f1651740520b7dfab23438d89bc8c0c0ab)
    
  * Self Driving Car
  
    1. [MIT Deep Learning for Self Driving Cars](http://selfdrivingcars.mit.edu/)
    2. [Self Driving Car Nano Degree](https://in.udacity.com/course/self-driving-car-engineer-nanodegree--nd013/)
   
  * Scientific Computing
   
    1. [Scientific Computing](http://academictorrents.com/details/6f7e43052129b95f470d3043cfce2bf5c15ae380)
    2. [High Performance Scientific Computing](http://academictorrents.com/details/cb91a3d7a4c4c086be240b54e83ed8d587b31ff5)
 
 
 

 
### Bonus Material

General Neural Network References:

### Books/Guides on Deep/Machine Learning: (all excellent)
 
  1. http://neuralnetworksanddeeplearning.com

  2. http://machinelearningmastery.com

  3. https://www.deeplearningbook.org

### Hacker’s Guide to Neural Nets by karpathy(My Favourite)

### Tutorials/Videos:

  1. [Youtube Playlist on “Deep Learning”, t from Oxford U. by Nando de Freitas](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)

  2. [Andrew Ng’s online course on ML at Stanford comes highly recommended](http://www.youtube.com/view_play_list?p=A89DCFA6ADACE599)

  3. [Stanford Tutorial:](http://ufldl.stanford.edu/tutorial/)

### Concepts in NN/Deep Learning:

  1. [Backpropagation](neuralnetworksanddeeplearning.org book), chapter 2

  2. [Chris Olah on backprop](http://colah.github.io/posts/2015-08-Backprop/)

  3. [Karpathy on backprop](http://cs231n.github.io/optimization-2/)

### Recurrent Neural Networks (RNN) (which mostly feature LSTM nowadays):

  1. [Karpathy post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 

  2. [Karpathy talk](https://www.youtube.com/watch?v=yCC09vCHzF8)

  3. [Excellent annotated Char-NN in Keras tutorial](https://github.com/ml4a/ml4a-guides/blob/master/notebooks/recurrent_neural_networks.ipynb)

  4. [Andrew Trask post/tutorial](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)

  5. [Denny Britz post](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)

  6. [Class notes/tutorial (long!)](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf)

  7. [CS class notes (short)](https://www.willamette.edu/~gorr/classes/cs449/rnn1.html)

  8. [Excellent post by Ross Goodwin RNNs](https://medium.com/@rossgoodwin/adventures-in-narrated-reality-6516ff395ba3#.q2xh8dp5t)

  9. [Great List of references; ](https://handong1587.github.io/deep_learning/2015/10/09/rnn-and-lstm.html)

  10.[RNN in TensorFlow](https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html)

  11. [Theano tutorial](http://deeplearning.net/tutorial/rnnslu.html)

  12. [Batch Normalization to the hidden-to-hidden transitions of our RNNs ](https://arxiv.org/abs/1510.01378) 


### Traditional RNNs suffer from vanishing/exploding gradient. Hence LSTM & others…

### Long Short-Term Memory (LSTM):

  1. [Tutorial](http://nbviewer.jupyter.org/github/JonathanRaiman/theano_lstm/blob/master/Tutorial.ipynb)

  2. [Chris Olah post](http://colah.github.io/posts/2015-08-Understanding-LSTMs)

  3. [Zach Lipton post, “Demystifying LSTM” (with Tutorial theano code)](http://blog.terminal.com/demistifying-long-short-term-memory-lstm-recurrent-neural-networks/)

  4. [Demo: Lightweight Theano-LSTM](https://github.com/JonathanRaiman/theano_lstm)

  5. [Massive 33-page review article by Lipton et al](http://arxiv.org/abs/1506.00019)

  6. [LSTM tutorial in Tensorflow](https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html)

  7. [Stateful LSTM in Keras for time-series prediction](https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py)

  8. [Much-need Docs on stateful LSTM in Keras](http://philipperemy.github.io/keras-stateful-lstm/)

  9. [Tensorflow sequence prediction](http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html)
