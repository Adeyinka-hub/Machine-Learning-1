{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:32:07.316047Z",
     "start_time": "2018-09-02T10:32:06.070691Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:32:08.154555Z",
     "start_time": "2018-09-02T10:32:07.977036Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:32:11.077748Z",
     "start_time": "2018-09-02T10:32:08.605532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_text(s):\n",
    "    import re\n",
    "    s = re.sub(r\"[^A-Za-z0-9^,\\*+-=]\", \" \",s)\n",
    "    s = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", s) #expand 'k' to '000' eg. 50k to 50000\n",
    "    s = re.sub(r\"\\;\",\" \",s)\n",
    "    s = re.sub(r\"\\:\",\" \",s)\n",
    "    s = re.sub(r\"\\,\",\" \",s)\n",
    "    s = re.sub(r\"\\.\",\" \",s)\n",
    "    s = re.sub(r\"\\<\",\" \",s)\n",
    "    s = re.sub(r\"\\^\",\" \",s)\n",
    "    s = re.sub(r\"(\\d+)(/)\", \"\\g<1> divide \", s) #change number/number to number divide number (eg. 2/3 to 2 divide 3)\n",
    "    s = re.sub(r\"\\/\",\" \",s) #replace the rest of / with white space\n",
    "    s = re.sub(r\"\\+\", \" plus \", s)\n",
    "    s = re.sub(r\"\\-\", \" minus \", s)\n",
    "    s = re.sub(r\"\\*\", \" multiply \", s)\n",
    "    s = re.sub(r\"\\=\", \"equal\", s)\n",
    "    s = re.sub(r\"What's\", \"What is \", s)\n",
    "    s = re.sub(r\"what's\", \"what is \", s)\n",
    "    s = re.sub(r\"Who's\", \"Who is \", s)\n",
    "    s = re.sub(r\"who's\", \"who is \", s)\n",
    "    s = re.sub(r\"\\'s\", \" \", s)\n",
    "    s = re.sub(r\"\\'ve\", \" have \", s)\n",
    "    s = re.sub(r\"can't\", \"cannot \", s)\n",
    "    s = re.sub(r\"n't\", \" not \", s)\n",
    "    s = re.sub(r\"\\'re\", \" are \", s)\n",
    "    s = re.sub(r\"\\'d\", \" would \", s)\n",
    "    s = re.sub(r\"\\'ll\", \" will \", s)\n",
    "    s = re.sub(r\"'m\", \" am \", s)\n",
    "    s = re.sub(r\"or not\", \" \", s)\n",
    "    s = re.sub(r\"What should I do to\", \"How can I\", s)\n",
    "    s = re.sub(r\"How do I\", \"How can I\", s)\n",
    "    s = re.sub(r\"How can you make\", \"What can make\", s)\n",
    "    s = re.sub(r\"How do we\", \"How do I\", s)\n",
    "    s = re.sub(r\"How do you\", \"How do I\", s)\n",
    "    s = re.sub(r\"Is it possible\", \"Can we\", s)\n",
    "    s = re.sub(r\"Why is\", \"Why\", s)\n",
    "    s = re.sub(r\"Which are\", \"What are\", s)\n",
    "    s = re.sub(r\"What are the reasons\", \"Why\", s)\n",
    "    s = re.sub(r\"What are some tips\", \"tips\", s)\n",
    "    s = re.sub(r\"What is the best way\", \"best way\", s)\n",
    "    s = re.sub(r\"e-mail\", \"email\", s)\n",
    "    s = re.sub(r\"e - mail\", \"email\", s)\n",
    "    s = re.sub(r\"US\", \"America\", s)\n",
    "    s = re.sub(r\"USA\", \"America\", s)\n",
    "    s = re.sub(r\"us\", \"America\", s)\n",
    "    s = re.sub(r\"usa\", \"America\", s)\n",
    "    s = re.sub(r\"Chinese\", \"China\", s)\n",
    "    s = re.sub(r\"india\", \"India\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s) #remove extra white space\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def remove_stopwords(string):\n",
    "    word_list = [word.lower() for word in string.split()]\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords_list = list(stopwords.words(\"english\"))\n",
    "    for word in word_list:\n",
    "        if word in stopwords_list:\n",
    "            word_list.remove(word)\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "def get_char_length_ratio(row):\n",
    "    return len(row['tweet'])/max(1,len(row['tweet_without_stopwords']))\n",
    "\n",
    "def get_synonyms(word):\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    synonyms = []\n",
    "    if wn.synsets(word):\n",
    "        for syn in wn.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    return list(set(synonyms))\n",
    "\n",
    "def get_row_syn_set(row):\n",
    "    import nltk\n",
    "    syn_set = [nltk.word_tokenize(row)]\n",
    "    for token in nltk.word_tokenize(row):\n",
    "        if get_synonyms(token):\n",
    "            syn_set.append(get_synonyms(token))\n",
    "    return set([y for x in syn_set for y in x])\n",
    "\n",
    "def get_Levenshtein(string1,string2):\n",
    "    import editdistance\n",
    "    return editdistance.eval(string1,string2)\n",
    "\n",
    "def num_pos(sent):\n",
    "    num_pos = 0\n",
    "    word_list = [word.lower() for word in nltk.word_tokenize(sent)]\n",
    "    for index, word in enumerate(word_list):\n",
    "        if word in positive_words:\n",
    "            if word_list[index-1] not in ['not','no']:\n",
    "                num_pos += 1\n",
    "    return num_pos\n",
    "\n",
    "def num_neg(sent):\n",
    "    num_neg = 0\n",
    "    word_list = [word.lower() for word in nltk.word_tokenize(sent)]\n",
    "    for index, word in enumerate(word_list):\n",
    "        if word in negative_words:\n",
    "            if word_list[index-1] not in ['not','no']:\n",
    "                num_neg += 1\n",
    "    return num_neg\n",
    "\n",
    "p_url = 'http://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "n_url = 'http://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "\n",
    "import requests,nltk\n",
    "positive_words = requests.get(p_url).content.decode('latin-1')\n",
    "positive_words = nltk.word_tokenize(positive_words)\n",
    "positive_words.remove('not')\n",
    "negative_words = requests.get(n_url).content.decode('latin-1')\n",
    "negative_words = nltk.word_tokenize(negative_words)\n",
    "positive_words = positive_words[413:]\n",
    "negative_words = negative_words[418:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:32:59.231738Z",
     "start_time": "2018-09-02T10:32:11.077748Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].map(lambda x: preprocessing_text(x))\n",
    "test['tweet'] = test['tweet'].map(lambda x: preprocessing_text(x))\n",
    "\n",
    "train['tweet'] = train['tweet'].astype(str)\n",
    "train['tweet_without_stopwords'] = train['tweet'].apply(lambda x: remove_stopwords(x))\n",
    "test['tweet'] = test['tweet'].astype(str)\n",
    "test['tweet_without_stopwords'] = test['tweet'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "train['char_length_ratio'] = train.apply(lambda row: get_char_length_ratio(row), axis=1)\n",
    "test['char_length_ratio'] = test.apply(lambda row: get_char_length_ratio(row), axis=1)\n",
    "\n",
    "train['tweet_tokens_syn_set'] = train['tweet_without_stopwords'].map(lambda row: get_row_syn_set(row))\n",
    "train['num_syn_words'] = train.apply(lambda x: len(x['tweet_tokens_syn_set'].intersection(set(nltk.word_tokenize(x['tweet'])))), axis=1)\n",
    "test['tweet_tokens_syn_set'] = test['tweet_without_stopwords'].map(lambda row: get_row_syn_set(row))\n",
    "test['num_syn_words'] = test.apply(lambda x:len(x['tweet_tokens_syn_set'].intersection(set(nltk.word_tokenize(x['tweet'])))), axis=1)\n",
    "\n",
    "train['Lev_dist'] = train.apply(lambda row: get_Levenshtein(row['tweet'],row['tweet_without_stopwords']),axis = 1)\n",
    "test['Lev_dist'] = test.apply(lambda row: get_Levenshtein(row['tweet'],row['tweet_without_stopwords']),axis = 1)\n",
    "\n",
    "train['tweet_num_pos'] = train['tweet_without_stopwords'].apply(lambda x: num_pos(x))\n",
    "train['tweet_num_neg'] = train['tweet_without_stopwords'].apply(lambda x: num_neg(x))\n",
    "\n",
    "test['tweet_num_pos'] = test['tweet_without_stopwords'].apply(lambda x: num_pos(x))\n",
    "test['tweet_num_neg'] = test['tweet_without_stopwords'].apply(lambda x: num_neg(x))\n",
    "\n",
    "train['tweet_diff_num'] = (train['tweet_num_pos'] - train['tweet_num_neg']).abs()\n",
    "test['tweet_diff_num'] = (test['tweet_num_pos'] - test['tweet_num_neg']).abs()\n",
    "\n",
    "train.drop('tweet_tokens_syn_set', axis=1, inplace=True)\n",
    "test.drop('tweet_tokens_syn_set', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:32:59.240113Z",
     "start_time": "2018-09-02T10:32:59.231738Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:34:06.398027Z",
     "start_time": "2018-09-02T10:34:06.358028Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = train.label.values\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.tweet_without_stopwords.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:34:11.221292Z",
     "start_time": "2018-09-02T10:34:09.204757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 4), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "xtest_tfv = tfv.transform(test.tweet_without_stopwords.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:34:13.374247Z",
     "start_time": "2018-09-02T10:34:11.459270Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=200)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "xtest_svd = svd.transform(xtest_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:34:16.176524Z",
     "start_time": "2018-09-02T10:34:14.306298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 4), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)\n",
    "xtest_ctv = ctv.transform(test.tweet_without_stopwords.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:39:18.329320Z",
     "start_time": "2018-09-02T10:38:35.429925Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.800 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict(xvalid_svd_scl)\n",
    "\n",
    "print (\"f1 score: %0.3f \" % f1_score(yvalid, predictions))\n",
    "predictions_test = clf.predict_proba(xtest_svd_scl)\n",
    "predictions_test = np.where(predictions_test[:,1]>=0.405, 1,0)\n",
    "sample['label'] = predictions_test\n",
    "sample.to_csv('svc_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:37:23.718345Z",
     "start_time": "2018-09-02T10:37:18.582856Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.777 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=8, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, n_jobs=-1, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict(xvalid_tfv.tocsc())\n",
    "\n",
    "print (\"f1 score: %0.3f \" % f1_score(yvalid, predictions))\n",
    "\n",
    "predictions_test = clf.predict_proba(xtest_tfv.tocsc())\n",
    "predictions_test = np.where(predictions_test[:,1]>=0.41, 1,0)\n",
    "sample['label'] = predictions_test\n",
    "sample.to_csv('xgb_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:41:06.826155Z",
     "start_time": "2018-09-02T10:41:06.810215Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(f1_score, greater_is_better=True, needs_proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:41:08.518940Z",
     "start_time": "2018-09-02T10:41:07.632479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this is the main ensembling class. how to use it is in the next cell!\n",
    "####################################################################### This Sciprt below to Kaggle.com\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
    "                 lower_is_better=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Ensembler init function\n",
    "        :param model_dict: model dictionary, see README for its format\n",
    "        :param num_folds: the number of folds for ensembling\n",
    "        :param task_type: classification or regression\n",
    "        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n",
    "        :param lower_is_better: is lower value of optimization function better or higher\n",
    "        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        \"\"\"\n",
    "        :param training_data: training data in tabular format\n",
    "        :param y: binary, multi-class or regression\n",
    "        :return: chain of models to be used in prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info(\"Found %d classes\", self.num_classes)\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "\n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
    "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
    "\n",
    "        for level in range(self.levels):\n",
    "\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "\n",
    "                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                        (model_num * self.num_classes):(model_num * self.num_classes) +\n",
    "                                                       self.num_classes] = temp_train_predictions\n",
    "\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
    "                                validation_score)\n",
    "                    foldnum += 1\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
    "                            avg_score, std_score)\n",
    "\n",
    "            logger.info(\"Saving predictions for level # %d\", level)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                        index=False, header=None)\n",
    "\n",
    "        return self.train_prediction_dict\n",
    "\n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
    "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
    "        self.test_data = test_data\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "\n",
    "                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "\n",
    "                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n",
    "                                                                                        self.num_classes] = temp_test_predictions\n",
    "\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                       index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:41:09.888717Z",
     "start_time": "2018-09-02T10:41:09.868718Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:41:11.114717Z",
     "start_time": "2018-09-02T10:41:11.094718Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:41:25.790591Z",
     "start_time": "2018-09-02T10:41:25.225789Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Number of words in the text ##\n",
    "train[\"num_words\"] = train[\"tweet\"].apply(lambda x: len(str(x).split()))\n",
    "test[\"num_words\"] = test[\"tweet\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train[\"num_unique_words\"] = train[\"tweet\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_unique_words\"] = test[\"tweet\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train[\"num_chars\"] = train[\"tweet\"].apply(lambda x: len(str(x)))\n",
    "test[\"num_chars\"] = test[\"tweet\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train[\"num_stopwords\"] = train[\"tweet\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test[\"num_stopwords\"] = test[\"tweet\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "train[\"num_punctuations\"] =train[\"tweet\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_punctuations\"] =test[\"tweet\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of upper case words in the text ##\n",
    "train[\"num_words_upper\"] = train[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test[\"num_words_upper\"] = test[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train[\"num_words_title\"] = train[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test[\"num_words_title\"] = test[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train[\"mean_word_len\"] = train[\"tweet\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"mean_word_len\"] = test[\"tweet\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T10:41:30.525930Z",
     "start_time": "2018-09-02T10:41:30.477925Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ['char_length_ratio',\n",
    "       'num_syn_words', 'Lev_dist', 'tweet_num_pos', 'tweet_num_neg',\n",
    "       'tweet_diff_num',\"num_words\", \"num_unique_words\", \"num_chars\", \"num_stopwords\", \"num_punctuations\", \"num_words_upper\", \"num_words_title\", \"mean_word_len\"]\n",
    "\n",
    "train_X = train[cols]\n",
    "test_X = test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:32:13.745789Z",
     "start_time": "2018-09-02T11:31:11.253243Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:11] INFO Found 2 classes\n",
      "[17:01:11] INFO Training Level 0 Fold # 1. Model # 0\n",
      "[17:01:22] INFO Predicting Level 0. Fold # 1. Model # 0\n",
      "[17:01:23] INFO Level 0. Fold # 1. Model # 0. Validation Score = 0.335918\n",
      "[17:01:23] INFO Training Level 0 Fold # 2. Model # 0\n",
      "[17:01:33] INFO Predicting Level 0. Fold # 2. Model # 0\n",
      "[17:01:34] INFO Level 0. Fold # 2. Model # 0. Validation Score = 0.285538\n",
      "[17:01:34] INFO Training Level 0 Fold # 3. Model # 0\n",
      "[17:01:44] INFO Predicting Level 0. Fold # 3. Model # 0\n",
      "[17:01:45] INFO Level 0. Fold # 3. Model # 0. Validation Score = 0.301440\n",
      "[17:01:45] INFO Level 0. Model # 0. Mean Score = 0.307632. Std Dev = 0.021029\n",
      "[17:01:45] INFO Training Level 0 Fold # 1. Model # 1\n",
      "[17:01:46] INFO Predicting Level 0. Fold # 1. Model # 1\n",
      "[17:01:46] INFO Level 0. Fold # 1. Model # 1. Validation Score = 0.309723\n",
      "[17:01:46] INFO Training Level 0 Fold # 2. Model # 1\n",
      "[17:01:46] INFO Predicting Level 0. Fold # 2. Model # 1\n",
      "[17:01:46] INFO Level 0. Fold # 2. Model # 1. Validation Score = 0.269141\n",
      "[17:01:46] INFO Training Level 0 Fold # 3. Model # 1\n",
      "[17:01:47] INFO Predicting Level 0. Fold # 3. Model # 1\n",
      "[17:01:47] INFO Level 0. Fold # 3. Model # 1. Validation Score = 0.293519\n",
      "[17:01:47] INFO Level 0. Model # 1. Mean Score = 0.290794. Std Dev = 0.016679\n",
      "[17:01:47] INFO Training Level 0 Fold # 1. Model # 2\n",
      "[17:01:51] INFO Predicting Level 0. Fold # 1. Model # 2\n",
      "[17:01:51] INFO Level 0. Fold # 1. Model # 2. Validation Score = 0.283444\n",
      "[17:01:51] INFO Training Level 0 Fold # 2. Model # 2\n",
      "[17:01:55] INFO Predicting Level 0. Fold # 2. Model # 2\n",
      "[17:01:55] INFO Level 0. Fold # 2. Model # 2. Validation Score = 0.262973\n",
      "[17:01:55] INFO Training Level 0 Fold # 3. Model # 2\n",
      "[17:02:00] INFO Predicting Level 0. Fold # 3. Model # 2\n",
      "[17:02:00] INFO Level 0. Fold # 3. Model # 2. Validation Score = 0.270856\n",
      "[17:02:00] INFO Level 0. Model # 2. Mean Score = 0.272424. Std Dev = 0.008431\n",
      "[17:02:00] INFO Training Level 0 Fold # 1. Model # 3\n",
      "[17:02:00] INFO Predicting Level 0. Fold # 1. Model # 3\n",
      "[17:02:00] INFO Level 0. Fold # 1. Model # 3. Validation Score = 8.691493\n",
      "[17:02:00] INFO Training Level 0 Fold # 2. Model # 3\n",
      "[17:02:00] INFO Predicting Level 0. Fold # 2. Model # 3\n",
      "[17:02:00] INFO Level 0. Fold # 2. Model # 3. Validation Score = 9.387439\n",
      "[17:02:00] INFO Training Level 0 Fold # 3. Model # 3\n",
      "[17:02:00] INFO Predicting Level 0. Fold # 3. Model # 3\n",
      "[17:02:00] INFO Level 0. Fold # 3. Model # 3. Validation Score = 8.697290\n",
      "[17:02:00] INFO Level 0. Model # 3. Mean Score = 8.925407. Std Dev = 0.326714\n",
      "[17:02:00] INFO Saving predictions for level # 0\n",
      "[17:02:00] INFO Training Level 1 Fold # 1. Model # 0\n",
      "[17:02:01] INFO Predicting Level 1. Fold # 1. Model # 0\n",
      "[17:02:01] INFO Level 1. Fold # 1. Model # 0. Validation Score = 0.292691\n",
      "[17:02:01] INFO Training Level 1 Fold # 2. Model # 0\n",
      "[17:02:02] INFO Predicting Level 1. Fold # 2. Model # 0\n",
      "[17:02:03] INFO Level 1. Fold # 2. Model # 0. Validation Score = 0.270917\n",
      "[17:02:03] INFO Training Level 1 Fold # 3. Model # 0\n",
      "[17:02:04] INFO Predicting Level 1. Fold # 3. Model # 0\n",
      "[17:02:04] INFO Level 1. Fold # 3. Model # 0. Validation Score = 0.281586\n",
      "[17:02:04] INFO Level 1. Model # 0. Mean Score = 0.281731. Std Dev = 0.008890\n",
      "[17:02:04] INFO Training Level 1 Fold # 1. Model # 1\n",
      "[17:02:05] INFO Predicting Level 1. Fold # 1. Model # 1\n",
      "[17:02:05] INFO Level 1. Fold # 1. Model # 1. Validation Score = 0.296477\n",
      "[17:02:05] INFO Training Level 1 Fold # 2. Model # 1\n",
      "[17:02:06] INFO Predicting Level 1. Fold # 2. Model # 1\n",
      "[17:02:06] INFO Level 1. Fold # 2. Model # 1. Validation Score = 0.277753\n",
      "[17:02:06] INFO Training Level 1 Fold # 3. Model # 1\n",
      "[17:02:07] INFO Predicting Level 1. Fold # 3. Model # 1\n",
      "[17:02:07] INFO Level 1. Fold # 3. Model # 1. Validation Score = 0.286212\n",
      "[17:02:07] INFO Level 1. Model # 1. Mean Score = 0.286814. Std Dev = 0.007656\n",
      "[17:02:07] INFO Training Level 1 Fold # 1. Model # 2\n",
      "[17:02:08] INFO Predicting Level 1. Fold # 1. Model # 2\n",
      "[17:02:08] INFO Level 1. Fold # 1. Model # 2. Validation Score = 0.294685\n",
      "[17:02:08] INFO Training Level 1 Fold # 2. Model # 2\n",
      "[17:02:09] INFO Predicting Level 1. Fold # 2. Model # 2\n",
      "[17:02:09] INFO Level 1. Fold # 2. Model # 2. Validation Score = 0.274593\n",
      "[17:02:09] INFO Training Level 1 Fold # 3. Model # 2\n",
      "[17:02:10] INFO Predicting Level 1. Fold # 3. Model # 2\n",
      "[17:02:10] INFO Level 1. Fold # 3. Model # 2. Validation Score = 0.284617\n",
      "[17:02:10] INFO Level 1. Model # 2. Mean Score = 0.284632. Std Dev = 0.008203\n",
      "[17:02:10] INFO Training Level 1 Fold # 1. Model # 3\n",
      "[17:02:11] INFO Predicting Level 1. Fold # 1. Model # 3\n",
      "[17:02:11] INFO Level 1. Fold # 1. Model # 3. Validation Score = 0.284142\n",
      "[17:02:11] INFO Training Level 1 Fold # 2. Model # 3\n",
      "[17:02:12] INFO Predicting Level 1. Fold # 2. Model # 3\n",
      "[17:02:12] INFO Level 1. Fold # 2. Model # 3. Validation Score = 0.254376\n",
      "[17:02:12] INFO Training Level 1 Fold # 3. Model # 3\n",
      "[17:02:13] INFO Predicting Level 1. Fold # 3. Model # 3\n",
      "[17:02:13] INFO Level 1. Fold # 3. Model # 3. Validation Score = 0.266175\n",
      "[17:02:13] INFO Level 1. Model # 3. Mean Score = 0.268231. Std Dev = 0.012239\n",
      "[17:02:13] INFO Saving predictions for level # 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: array([[  9.42286218e-01,   5.77137818e-02,   9.99805711e-01, ...,\n",
       "           8.85082595e-03,   1.00000000e+00,   2.52200033e-92],\n",
       "        [  9.70445041e-01,   2.95549592e-02,   9.98932747e-01, ...,\n",
       "           8.55579413e-03,   7.18654629e-16,   1.00000000e+00],\n",
       "        [  6.34743234e-02,   9.36525677e-01,   3.47517494e-01, ...,\n",
       "           3.90822142e-01,   1.51577760e-25,   1.00000000e+00],\n",
       "        ..., \n",
       "        [  9.16477625e-01,   8.35223753e-02,   9.99999647e-01, ...,\n",
       "           2.98558129e-03,   1.00000000e+00,   6.80273006e-86],\n",
       "        [  8.66455855e-01,   1.33544145e-01,   9.90745144e-01, ...,\n",
       "           5.04983328e-02,   1.04948717e-07,   9.99999895e-01],\n",
       "        [  7.55104872e-03,   9.92448951e-01,   2.64850446e-02, ...,\n",
       "           7.63928294e-01,   1.44059087e-17,   1.00000000e+00]]),\n",
       " 1: array([[  9.52074421e-01,   4.79255788e-02,   9.58195149e-01, ...,\n",
       "           4.55324932e-02,   9.99392152e-01,   6.07825583e-04],\n",
       "        [  9.70454063e-01,   2.95459367e-02,   9.54415407e-01, ...,\n",
       "           3.65875520e-02,   9.97906983e-01,   2.09303363e-03],\n",
       "        [  2.51692893e-01,   7.48307107e-01,   2.38694091e-01, ...,\n",
       "           7.51031222e-01,   1.72431529e-01,   8.27568471e-01],\n",
       "        ..., \n",
       "        [  9.53999115e-01,   4.60008851e-02,   9.59065266e-01, ...,\n",
       "           4.47468298e-02,   9.99341369e-01,   6.58636214e-04],\n",
       "        [  9.65884051e-01,   3.41159487e-02,   9.60644513e-01, ...,\n",
       "           3.49709151e-02,   9.34725642e-01,   6.52743727e-02],\n",
       "        [  1.81446211e-01,   8.18553789e-01,   1.89758722e-01, ...,\n",
       "           8.10529066e-01,   4.97666001e-02,   9.50233400e-01]])}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the data to be used for every level of ensembling:\n",
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv, train_X.values,], 1: [xtrain_tfv]}\n",
    "test_data_dict = {0: [xtest_tfv, xtest_ctv, xtest_tfv, xtest_ctv, test_X.values], 1: [xtest_tfv]}\n",
    "\n",
    "model_dict = {0: [SVC(C=1.5,probability=True),LogisticRegression(C=5),xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)\\\n",
    "                  ,MultinomialNB(alpha=0.1)],\n",
    "\n",
    "              1: [SVC(C=1,probability=True),SVC(C=5,probability=True),\\\n",
    "                  SVC(C=2,probability=True), xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
    "\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='./temp//')\n",
    "\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_tfv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:32:57.464922Z",
     "start_time": "2018-09-02T11:32:13.747093Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:02:13] INFO Training Fulldata Level 0. Model # 0\n",
      "[17:02:39] INFO Predicting Test Level 0. Model # 0\n",
      "[17:02:40] INFO Training Fulldata Level 0. Model # 1\n",
      "[17:02:41] INFO Predicting Test Level 0. Model # 1\n",
      "[17:02:41] INFO Training Fulldata Level 0. Model # 2\n",
      "[17:02:47] INFO Predicting Test Level 0. Model # 2\n",
      "[17:02:47] INFO Training Fulldata Level 0. Model # 3\n",
      "[17:02:47] INFO Predicting Test Level 0. Model # 3\n",
      "[17:02:47] INFO Training Fulldata Level 1. Model # 0\n",
      "[17:02:50] INFO Predicting Test Level 1. Model # 0\n",
      "[17:02:50] INFO Training Fulldata Level 1. Model # 1\n",
      "[17:02:53] INFO Predicting Test Level 1. Model # 1\n",
      "[17:02:53] INFO Training Fulldata Level 1. Model # 2\n",
      "[17:02:56] INFO Predicting Test Level 1. Model # 2\n",
      "[17:02:56] INFO Training Fulldata Level 1. Model # 3\n",
      "[17:02:57] INFO Predicting Test Level 1. Model # 3\n"
     ]
    }
   ],
   "source": [
    "preds = ens.predict(test_data_dict, lentest=xtest_tfv.shape[0])\n",
    "predictions_test = preds[1][:,1]\n",
    "predictions_test = np.where(predictions_test>=0.41, 1,0)\n",
    "sample['label'] = predictions_test\n",
    "sample.to_csv('ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:13:48.692094Z",
     "start_time": "2018-09-02T12:13:48.680127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dependecy imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import SpatialDropout1D, Dropout\n",
    "\n",
    "from keras.layers import Dense, Activation, Reshape, Merge, Embedding, Input, Concatenate\n",
    "from keras.models import Model as KerasModel\n",
    "\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:46:44.890287Z",
     "start_time": "2018-09-02T11:46:44.623769Z"
    }
   },
   "outputs": [],
   "source": [
    "max_fatures = 2500 # Top 2000 words\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "# The training phase is by means of the fit_on_texts method and you\n",
    "# can see the word index using the word_index property:\n",
    "tokenizer.fit_on_texts(train['tweet_without_stopwords'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:46:50.661973Z",
     "start_time": "2018-09-02T11:46:50.503889Z"
    }
   },
   "outputs": [],
   "source": [
    "# texts_to_sequences method turns input into numerical arrays\n",
    "train_data = tokenizer.texts_to_sequences(train['tweet_without_stopwords'].values)\n",
    "test_data = tokenizer.texts_to_sequences(test['tweet_without_stopwords'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:46:56.377727Z",
     "start_time": "2018-09-02T11:46:56.329727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples:\n",
      "dreamy effect created straight the iphone impressive spring flower blossom http instagram com p tpogyugzze --> [1432, 2143, 13, 1, 519, 530, 2, 7, 3, 6]\n",
      "woohoo finally get nx1000 finally camera nx1000 samsung excited nice black peace tha http instagr p sa8b nzddg --> [76, 43, 76, 109, 8, 185, 188, 160, 514, 1918, 2, 16, 6]\n",
      "fibs rhymes sibs nibs jibs bibs ribs dibs start today http bit ly rhymeapp rhyme iphone --> [561, 476, 38, 2, 54, 33, 562, 563, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExamples:\")\n",
    "print(train['tweet_without_stopwords'][100], '-->', train_data[100])\n",
    "print(train['tweet_without_stopwords'][200], '-->', train_data[200])\n",
    "print(train['tweet_without_stopwords'][300], '-->', train_data[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:47:34.739728Z",
     "start_time": "2018-09-02T11:47:34.603729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example\n",
      "[1432, 2143, 13, 1, 519, 530, 2, 7, 3, 6] --> [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0 1432 2143\n",
      "   13    1  519  530    2    7    3    6]\n"
     ]
    }
   ],
   "source": [
    "# All Phrase numerical values reshape to match size for all\n",
    "train_data_pad = pad_sequences(train_data,maxlen=68)\n",
    "test_data_pad = pad_sequences(test_data)\n",
    "print(\"\\nExample\")\n",
    "print(train_data[100], '-->', train_data_pad[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:47:35.347664Z",
     "start_time": "2018-09-02T11:47:35.279676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input train data shape: (7920, 68)\n",
      "Input test data shape: (1953, 68)\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput train data shape:', train_data_pad.shape)\n",
    "print('Input test data shape:', test_data_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:47:40.446558Z",
     "start_time": "2018-09-02T11:47:40.398621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample labels:\n",
      "[[1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# One Hot encoding\n",
    "train_labels = pd.get_dummies(train['label']).values\n",
    "print('Sample labels:')\n",
    "print(train_labels[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:49:39.845276Z",
     "start_time": "2018-09-02T11:49:39.825270Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_dim = 200\n",
    "lstm_out = 150 # Output Neurons\n",
    "batch_size = 128\n",
    "drop_out = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:11:45.408690Z",
     "start_time": "2018-09-02T12:11:45.392738Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X_train, y_train):\n",
    "    self.model.fit(self.preprocessing(X_train), y_train, epochs=self.epochs, batch_size=512)\n",
    "\n",
    "def guess(self, features):\n",
    "        features = self.preprocessing(features)\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:47:34.312318Z",
     "start_time": "2018-09-02T12:47:34.291385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 68)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:48:00.310118Z",
     "start_time": "2018-09-02T12:48:00.302140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 14)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:04:01.719203Z",
     "start_time": "2018-09-02T13:04:01.715212Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:04:22.430978Z",
     "start_time": "2018-09-02T13:04:22.410037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:04:49.471674Z",
     "start_time": "2018-09-02T13:04:49.448779Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_train_X = scaler.transform(train_X)\n",
    "scaled_test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:05:45.287532Z",
     "start_time": "2018-09-02T13:05:45.252628Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_features(X):\n",
    "    \n",
    "    X_list = []\n",
    "    x_0 = train_data_pad[..., :]\n",
    "    X_list.append(x_0)\n",
    "    \n",
    "    x_1 = X[..., :]\n",
    "    X_list.append(x_1)\n",
    "\n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:10:24.144636Z",
     "start_time": "2018-09-02T13:10:22.365740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 68)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_31 (Embedding)         (None, 68, 200)       500000      input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "spatial_dropout1d_29 (SpatialDro (None, 68, 200)       0           embedding_31[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 14)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                   (None, 68, 150)       210600      spatial_dropout1d_29[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_64 (Dense)                 (None, 64)            960         input_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                   (None, 50)            40200       lstm_35[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)              (None, 64)            0           dense_64[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)             (None, 50)            0           lstm_36[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_65 (Dense)                 (None, 32)            2080        reshape_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_63 (Dense)                 (None, 64)            3264        dropout_41[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)             (None, 32)            0           dense_65[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 96)            0           dense_63[0][0]                   \n",
      "                                                                   reshape_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_66 (Dense)                 (None, 16)            1552        concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)             (None, 16)            0           dense_66[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_67 (Dense)                 (None, 2)             34          dropout_42[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 758,690\n",
      "Trainable params: 758,690\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp_1 = Input(shape=(68,))\n",
    "out_1 = Embedding(max_fatures, embed_dim, input_length=train_data_pad.shape[1])(inp_1)\n",
    "s_drop_1 = SpatialDropout1D(.2)(out_1)\n",
    "lstm_1 = LSTM(lstm_out,return_sequences=True, dropout = 0.2)(s_drop_1)\n",
    "lstm_2 = LSTM(50, dropout=0.2)(lstm_1)\n",
    "drop_2 = Dropout(drop_out)(lstm_2)\n",
    "out_dense_1 = Dense(64,activation='relu')(drop_2)\n",
    "\n",
    "inp_2 = Input(shape=(14,))\n",
    "dense_2 = Dense(64,activation='relu')(inp_2)\n",
    "out_dense_2 = Reshape(target_shape=(64,))(dense_2)\n",
    "dense_3 = Dense(32,activation='relu')(out_dense_2)\n",
    "out_dense_3 = Reshape(target_shape=(32,))(dense_3)\n",
    "\n",
    "input_model = [inp_1, inp_2]\n",
    "output_model = [out_dense_1, out_dense_3]\n",
    "\n",
    "output = Concatenate()(output_model)\n",
    "output = Dense(16, activation='relu')(output)\n",
    "output = Dropout(drop_out)(output)\n",
    "output = Dense(2, activation='softmax')(output)\n",
    "\n",
    "model = KerasModel(inputs=input_model, outputs=output)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:16:32.398592Z",
     "start_time": "2018-09-02T13:12:51.545105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21s - loss: 0.1701 - acc: 0.9407\n",
      "Epoch 2/10\n",
      "21s - loss: 0.1496 - acc: 0.9511\n",
      "Epoch 3/10\n",
      "21s - loss: 0.1369 - acc: 0.9578\n",
      "Epoch 4/10\n",
      "21s - loss: 0.1280 - acc: 0.9645\n",
      "Epoch 5/10\n",
      "21s - loss: 0.1254 - acc: 0.9620\n",
      "Epoch 6/10\n",
      "21s - loss: 0.1046 - acc: 0.9718\n",
      "Epoch 7/10\n",
      "23s - loss: 0.1044 - acc: 0.9734\n",
      "Epoch 8/10\n",
      "22s - loss: 0.0958 - acc: 0.9745\n",
      "Epoch 9/10\n",
      "22s - loss: 0.0864 - acc: 0.9792\n",
      "Epoch 10/10\n",
      "22s - loss: 0.0792 - acc: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25ccdb01f28>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(split_features(train_X.values), train_labels, epochs=10, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:16:32.410560Z",
     "start_time": "2018-09-02T13:16:32.401583Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_features(X):\n",
    "    \n",
    "    X_list = []\n",
    "    x_0 = test_data_pad[..., :]\n",
    "    X_list.append(x_0)\n",
    "    \n",
    "    x_1 = X[..., :]\n",
    "    X_list.append(x_1)\n",
    "\n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:16:39.385817Z",
     "start_time": "2018-09-02T13:16:32.415543Z"
    }
   },
   "outputs": [],
   "source": [
    "result = model.predict(split_features(test_X.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:16:39.393774Z",
     "start_time": "2018-09-02T13:16:39.387793Z"
    }
   },
   "outputs": [],
   "source": [
    "result = result[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:16:39.674999Z",
     "start_time": "2018-09-02T13:16:39.396770Z"
    }
   },
   "outputs": [],
   "source": [
    "sample['label'] = result\n",
    "sample.to_csv('_a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:16:39.888598Z",
     "start_time": "2018-09-02T13:16:39.678984Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_test = np.where(result>=0.41, 1,0)\n",
    "sample['label'] = predictions_test\n",
    "sample.to_csv('__a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:16:40.381062Z",
     "start_time": "2018-09-02T13:16:39.896384Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(max_fatures, embed_dim, input_length=train_data_pad.shape[1]))\n",
    "# model.add(SpatialDropout1D(drop_out))\n",
    "\n",
    "# # LSTMs\n",
    "# model.add(LSTM(lstm_out,return_sequences=True, dropout = 0.2))\n",
    "# model.add(LSTM(50, dropout=0.2))\n",
    "# model.add(Dropout(drop_out))\n",
    "\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "# model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:09:40.864542Z",
     "start_time": "2018-09-02T12:07:29.319734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6336 samples, validate on 1584 samples\n",
      "Epoch 1/6\n",
      "22s - loss: 0.3987 - acc: 0.8174 - val_loss: 0.2621 - val_acc: 0.8826\n",
      "Epoch 2/6\n",
      "21s - loss: 0.2250 - acc: 0.9058 - val_loss: 0.2379 - val_acc: 0.8939\n",
      "Epoch 3/6\n",
      "19s - loss: 0.1781 - acc: 0.9306 - val_loss: 0.2584 - val_acc: 0.8826\n",
      "Epoch 4/6\n",
      "20s - loss: 0.1465 - acc: 0.9426 - val_loss: 0.2943 - val_acc: 0.8782\n",
      "Epoch 5/6\n",
      "21s - loss: 0.1368 - acc: 0.9485 - val_loss: 0.3084 - val_acc: 0.8756\n",
      "Epoch 6/6\n",
      "21s - loss: 0.1204 - acc: 0.9568 - val_loss: 0.3744 - val_acc: 0.8712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25cdd5655f8>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_pad, train_labels, epochs=10, batch_size=batch_size, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:09:47.408977Z",
     "start_time": "2018-09-02T12:09:40.870522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953/1953 [==============================] - ETA: 78 - ETA: 41 - ETA: 28 - ETA: 22 - ETA: 18 - ETA: 15 - ETA: 14 - ETA: 12 - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 6s     \n"
     ]
    }
   ],
   "source": [
    "preds_1 = model.predict_proba(test_data_pad)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:59:28.861522Z",
     "start_time": "2018-09-02T11:56:47.320267Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6336 samples, validate on 1584 samples\n",
      "Epoch 1/10\n",
      "17s - loss: 0.6530 - acc: 0.7331 - val_loss: 0.6125 - val_acc: 0.7519\n",
      "Epoch 2/10\n",
      "15s - loss: 0.5993 - acc: 0.7423 - val_loss: 0.5776 - val_acc: 0.7519\n",
      "Epoch 3/10\n",
      "15s - loss: 0.5788 - acc: 0.7423 - val_loss: 0.5654 - val_acc: 0.7519\n",
      "Epoch 4/10\n",
      "15s - loss: 0.5721 - acc: 0.7423 - val_loss: 0.5607 - val_acc: 0.7519\n",
      "Epoch 5/10\n",
      "15s - loss: 0.5696 - acc: 0.7423 - val_loss: 0.5587 - val_acc: 0.7519\n",
      "Epoch 6/10\n",
      "15s - loss: 0.5689 - acc: 0.7423 - val_loss: 0.5577 - val_acc: 0.7519\n",
      "Epoch 7/10\n",
      "15s - loss: 0.5685 - acc: 0.7423 - val_loss: 0.5569 - val_acc: 0.7519\n",
      "Epoch 8/10\n",
      "15s - loss: 0.5671 - acc: 0.7423 - val_loss: 0.5562 - val_acc: 0.7519\n",
      "Epoch 9/10\n",
      "15s - loss: 0.5667 - acc: 0.7423 - val_loss: 0.5556 - val_acc: 0.7519\n",
      "Epoch 10/10\n",
      "15s - loss: 0.5667 - acc: 0.7423 - val_loss: 0.5550 - val_acc: 0.7519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25ccb7e1eb8>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple LSTM with two dense layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(max_fatures, embed_dim, input_length=train_data_pad.shape[1]))\n",
    "model_2.add(SpatialDropout1D(0.3))\n",
    "model_2.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model_2.add(Dense(512, activation='relu'))\n",
    "model_2.add(Dropout(0.3))\n",
    "\n",
    "model_2.add(Dense(256, activation='relu'))\n",
    "model_2.add(Dropout(0.3))\n",
    "\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model_2 with early stopping callback\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=2, mode='auto')\n",
    "model_2.fit(train_data_pad, train_labels, epochs=10, batch_size=batch_size, verbose=2, validation_split=0.2,callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:59:28.953523Z",
     "start_time": "2018-09-02T11:59:28.861522Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 68, 200)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_17 (Spatia (None, 68, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 300)               601200    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,387,154\n",
      "Trainable params: 1,387,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:59:33.796216Z",
     "start_time": "2018-09-02T11:59:28.953523Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1952/1953 [============================>.] - ETA: 49 - ETA: 26 - ETA: 18 - ETA: 14 - ETA: 12 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA: 0s"
     ]
    }
   ],
   "source": [
    "preds_2 = model_2.predict_proba(test_data_pad)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:04:15.906486Z",
     "start_time": "2018-09-02T12:00:46.391942Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6336 samples, validate on 1584 samples\n",
      "Epoch 1/10\n",
      "36s - loss: 0.3734 - val_loss: 0.2472\n",
      "Epoch 2/10\n",
      "33s - loss: 0.2249 - val_loss: 0.2318\n",
      "Epoch 3/10\n",
      "33s - loss: 0.1813 - val_loss: 0.2470\n",
      "Epoch 4/10\n",
      "33s - loss: 0.1651 - val_loss: 0.2776\n",
      "Epoch 5/10\n",
      "33s - loss: 0.1390 - val_loss: 0.3069\n",
      "Epoch 6/10\n",
      "32s - loss: 0.1168 - val_loss: 0.3684\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25cd949ef60>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "from keras.layers import Bidirectional\n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(max_fatures, embed_dim, input_length=train_data_pad.shape[1]))\n",
    "model_3.add(SpatialDropout1D(0.3))\n",
    "model_3.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model_3.add(Dense(512, activation='relu'))\n",
    "model_3.add(Dropout(0.3))\n",
    "\n",
    "model_3.add(Dense(256, activation='relu'))\n",
    "model_3.add(Dropout(0.3))\n",
    "\n",
    "model_3.add(Dense(2, activation='softmax'))\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=2, mode='auto')\n",
    "model_3.fit(train_data_pad, train_labels, epochs=10, batch_size=batch_size, verbose=2, validation_split=0.2,callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:04:15.994485Z",
     "start_time": "2018-09-02T12:04:15.906486Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 68, 200)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_20 (Spatia (None, 68, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 600)               1202400   \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               307712    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,141,954\n",
      "Trainable params: 2,141,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:04:31.831129Z",
     "start_time": "2018-09-02T12:04:23.544793Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953/1953 [==============================] - ETA: 69 - ETA: 37 - ETA: 26 - ETA: 21 - ETA: 17 - ETA: 15 - ETA: 14 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 8s     \n"
     ]
    }
   ],
   "source": [
    "preds_3 = model_3.predict_proba(test_data_pad)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### all_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:04:36.813147Z",
     "start_time": "2018-09-02T12:04:36.613204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_test = np.where(preds_1>=0.41, 1,0)\n",
    "sample['label'] = predictions_test\n",
    "sample.to_csv('pred_lstm1.csv', index=False)\n",
    "\n",
    "predictions_test = np.where(preds_2>=0.41, 1,0)\n",
    "sample['label'] = predictions_test\n",
    "sample.to_csv('pred_lstm2.csv', index=False)\n",
    "\n",
    "predictions_test = np.where(preds_3>=0.41, 1,0)\n",
    "sample['label'] = predictions_test\n",
    "sample.to_csv('pred_lstm3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T12:09:47.427929Z",
     "start_time": "2018-09-02T12:09:47.411967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_test = np.where(preds_1>=0.41, 1,0)\n",
    "sample['label'] = predictions_test\n",
    "sample.to_csv('pred_lstmlstm1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
