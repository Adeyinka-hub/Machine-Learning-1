{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have done this as our first step, but better late than never. This simple kernel demonstrates how to run local feature validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "import gc, warnings, json\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintr = pd.read_csv('input/train_transaction.csv.zip')\n",
    "trainid = pd.read_csv('input/train_identity.csv.zip')\n",
    "testtr  = pd.read_csv('input/test_transaction.csv.zip')\n",
    "testid  = pd.read_csv('input/test_identity.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-12-01 00:00:00')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each categorical variable, we'd like to experiment with\n",
    "# the count of appearances within that day's hour\n",
    "# This will only work if the distributions (counts) are similar in train + test\n",
    "\n",
    "START_DATE     = '2017-11-30'\n",
    "startdate      = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "traintr['tdt']    = traintr['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "traintr['thour']  = traintr.tdt.dt.hour\n",
    "traintr['tmonth'] = (traintr.tdt.dt.year-2017) * 12 + traintr.tdt.dt.month\n",
    "traintr['tweek'] = 52 * (traintr.tdt.dt.year-2017) + traintr.tdt.dt.weekofyear\n",
    "traintr['tdoy'] = 365 * (traintr.tdt.dt.year-2017) + traintr.tdt.dt.dayofyear\n",
    "traintr.tdoy -= traintr.tdoy.min() # 0-offset\n",
    "\n",
    "testtr['tdt']    = testtr['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "testtr['thour']  = testtr.tdt.dt.hour\n",
    "testtr['tmonth'] = (testtr.tdt.dt.year-2017) * 12 + testtr.tdt.dt.month\n",
    "testtr['tweek'] = 52 * (testtr.tdt.dt.year-2017) + testtr.tdt.dt.weekofyear\n",
    "testtr['tdoy'] = 365 * (testtr.tdt.dt.year-2017) + testtr.tdt.dt.dayofyear\n",
    "\n",
    "traintr.tdt.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmonth\n",
       "12    137321\n",
       "13     92585\n",
       "14     86021\n",
       "15    101632\n",
       "16     83655\n",
       "17     89326\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintr.groupby('tmonth').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 181)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintr.tdoy.min(),traintr.tdoy.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2**8+1\n",
    "\n",
    "# TODO: Optimize\n",
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'boosting_type':'gbdt',\n",
    "    'metric':'auc',\n",
    "    'n_jobs':-1,\n",
    "    'learning_rate':0.01,\n",
    "    'num_leaves': 2**5, # 5-8\n",
    "    'max_depth':-1,\n",
    "    'tree_learner':'serial',\n",
    "    'colsample_bytree': 0.7,\n",
    "    'subsample_freq':1,\n",
    "    'subsample':0.7,\n",
    "    'max_bin':255,\n",
    "    'verbose':-1,\n",
    "    'seed': SEED,\n",
    "    'feature_fraction_seed': SEED + 2,\n",
    "    'bagging_seed': SEED + 3,\n",
    "    'drop_seed': SEED + 4,\n",
    "    'data_random_seed': SEED + 5,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_report(report):\n",
    "    print('{} Folds Used'.format(len(report['folds'])))\n",
    "    print('{} Neg DownSample Frac with {} Seed'.format(report['downsample_frac'], report['downsample_seed']))\n",
    "    print('{} AVG AUC, {} STD'.format(np.round(report['avg_auc'],3), np.round(report['std_auc'],3)))\n",
    "    print('{} AVG Rounds, {} Rounds'.format(report['avg_iterations'], report['std_iterations']), end='\\n\\n')\n",
    "\n",
    "    features = pd.DataFrame({\n",
    "        'feature': report['features'],\n",
    "        'adversarial': list(report['cvs'].values()),\n",
    "        'perm_import': list(report['avg_permutation_importance'].values()),\n",
    "        'perm_import_std': list(report['std_permutation_importance'].values()),\n",
    "    })\n",
    "    features.sort_values(['perm_import','adversarial'], ascending=False, inplace=True)\n",
    "    \n",
    "    sns_df = pd.DataFrame({\n",
    "        'feature' : sum([list(fold['permutation_importance'].keys()) for fold in results['folds']], []),\n",
    "        'perm_import': sum([list(fold['permutation_importance'].values()) for fold in results['folds']], []),\n",
    "        \n",
    "    })\n",
    "    sns_df.sort_values(['feature','perm_import'], ascending=False, inplace=True)\n",
    "    \n",
    "    print(report['params'])\n",
    "    return features, sns_df\n",
    "\n",
    "def compare_reports(report1, report2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(data, features, params, downsample_seed=None, downsample_frac=0.2, save_file_path=None):\n",
    "    # NOTE: data should contain, at minimal, all train + test samples,\n",
    "    # along with the isFraud column, for separation and scoring purposes.\n",
    "    gc.collect()\n",
    "    \n",
    "    # Run evaluation and store results in a report\n",
    "    # Steps:\n",
    "    # 1) [x] Negative Downsample non-frauds\n",
    "    # 2) [x] Run adversarial validation on features + record scores\n",
    "    # 3) [x] Train on 50% overlapping folds on the trainset\n",
    "    # 3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
    "    # 4) [x] Aggregate and save results\n",
    "    \n",
    "    report = {\n",
    "        'features': features,\n",
    "        'params': params,\n",
    "        'downsample_seed': downsample_seed,\n",
    "        'downsample_frac': downsample_frac,\n",
    "        'cvs': {},\n",
    "        'folds': [],\n",
    "        'avg_permutation_importance': {},\n",
    "        'std_permutation_importance': {},\n",
    "    }\n",
    "    \n",
    "    ######################\n",
    "    print('\\n# 1) [x] Negative Downsample (non-frauds)')\n",
    "    if downsample_seed is None:\n",
    "        selection = data.copy()\n",
    "    else:\n",
    "        np.random.seed(downsample_seed)\n",
    "        normies = data[data.isFraud==0].index.values\n",
    "        normies = np.random.choice(\n",
    "            normies,\n",
    "            int(data.shape[0]*downsample_frac),\n",
    "            replace=False\n",
    "        )\n",
    "        selection = data[data.index.isin(\n",
    "            # All fruds and a number of normies\n",
    "            np.concatenate([normies, data[data.isFraud==1].index.values])\n",
    "        )].copy()\n",
    "    print(selection.shape[0], 'total train samples!')\n",
    "    \n",
    "    if selection.shape[0] > data.isFraud.isna().sum():\n",
    "        # If we have more train samples than test samples, use all test samples\n",
    "        selection_test = data[data.isFraud.isna()]\n",
    "    else:\n",
    "        # Use a balanced set of test samples \n",
    "        selection_test = np.random.choice(\n",
    "            data[data.isFraud.isna()].index.values,\n",
    "            selection.shape[0],\n",
    "            replace=False\n",
    "        )\n",
    "        selection_test = data[data.index.isin(selection_test)]\n",
    "\n",
    "    ######################\n",
    "    print('\\n# 2) [x] Run adversarial validation (CVS) on features + record scores')\n",
    "    # Build CVS dataset\n",
    "    cvsdata = selection.append(selection_test, sort=False)\n",
    "    cvsdata.reset_index(inplace=True)\n",
    "    cvsdata['which_set'] = (np.arange(cvsdata.shape[0]) >= selection.shape[0]).astype(np.uint8)\n",
    "    cvsdata = cvsdata.sample(frac=1).reset_index(drop=True) # Shuffle the thing\n",
    "    trn_cvs = cvsdata.index < (cvsdata.shape[0] // 2)\n",
    "    for col in features:\n",
    "        trn_lgb = lgb.Dataset(cvsdata[trn_cvs][[col]], label=cvsdata[trn_cvs].which_set)\n",
    "        val_lgb = lgb.Dataset(cvsdata[~trn_cvs][[col]], label=cvsdata[~trn_cvs].which_set)\n",
    "        clf = lgb.train(\n",
    "            params,\n",
    "            trn_lgb,\n",
    "            valid_sets = [trn_lgb, val_lgb],\n",
    "            verbose_eval = 200,\n",
    "            early_stopping_rounds = 25,\n",
    "            num_boost_round = 80000,\n",
    "        )\n",
    "        report['cvs'][col] = clf.best_score['valid_1']['auc'] - 0.5 # 0.5 = 0, best score\n",
    "    del cvsdata, trn_lgb, val_lgb, trn_cvs; gc.collect()\n",
    "    \n",
    "    ######################\n",
    "    print('\\n#3) [x] Train on 50% overlapping folds on the trainset')\n",
    "    for fold_, i in enumerate(range(0,57,14)):\n",
    "        gc.collect()\n",
    "        fold = {\n",
    "            'fold_num': fold_,\n",
    "            'trn_range': [i,i+90],\n",
    "            'val_range': [i+90+15,i+90+15+20],\n",
    "        }\n",
    "        print('\\nFold', fold_+1, '— Train', fold['trn_range'], '— Test', fold['val_range'])\n",
    "        \n",
    "        trn = selection[selection.tdoy.between(i, 90+i)]\n",
    "        val = selection[selection.tdoy.between(90+i+15, 90+i+15+20)].copy()\n",
    "        trn_lgb = lgb.Dataset(trn[features], label=trn.isFraud)\n",
    "        val_lgb = lgb.Dataset(val[features], label=val.isFraud)\n",
    "        clf = lgb.train(\n",
    "            params,\n",
    "            trn_lgb,\n",
    "            valid_sets = [trn_lgb, val_lgb],\n",
    "            verbose_eval = 200,\n",
    "            early_stopping_rounds = 25,\n",
    "            num_boost_round = 80000,\n",
    "            #categorical_feature=[]\n",
    "        )\n",
    "        baseline = clf.best_score['valid_1']['auc']\n",
    "        fold['auc'] = baseline\n",
    "        fold['iterations'] = clf.best_iteration\n",
    "        print('baseline - ', baseline)\n",
    "        \n",
    "        ######################\n",
    "        # TODO: Repalce with Drop importance\n",
    "        print('\\n#3b) [x] Perform permutation importance (soon to be drop importance) each fold')\n",
    "        perm = {}\n",
    "        for col in features:\n",
    "            backup = val[col].values.copy()\n",
    "            val[col] = np.random.permutation(val[col].values)\n",
    "            \n",
    "            y_true = clf.predict(val[features])\n",
    "            perm[col] = baseline - roc_auc_score(val.isFraud, y_true)\n",
    "            val[col] = backup\n",
    "        fold['permutation_importance'] = perm\n",
    "        report['folds'].append(fold)\n",
    "    \n",
    "    ######################\n",
    "    print('\\n# 4) [x] Aggregate and save results')\n",
    "    aucs = [fold['auc'] for fold in report['folds']]\n",
    "    report['avg_auc'] = np.mean(aucs)\n",
    "    report['std_auc'] = np.std(aucs)\n",
    "\n",
    "    iterations = [fold['iterations'] for fold in report['folds']]\n",
    "    report['avg_iterations'] = np.mean(iterations)\n",
    "    report['std_iterations'] = np.std(iterations)\n",
    "\n",
    "    for feature in features:\n",
    "        pi = [fold['permutation_importance'][feature] for fold in report['folds']]\n",
    "        report['avg_permutation_importance'][feature] = np.mean(pi)\n",
    "        report['std_permutation_importance'][feature] = np.std(pi)\n",
    "\n",
    "    if save_file_path is not None:\n",
    "        with open(save_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    gc.collect()\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above does a few things. First, it downsamples the negative values. This was reported by a number of top-50 people on the forums, and gold medal winners in previous competitions have used this technique as well. The idea being that fraud is unique and it shouldn't matter much which non-frauds we train against, since the frauds are a distinct class and should have some level of 'difference' about them. Think outliers. This would also be a good time to mention that when we train for _submission_, we can add diversity to our models by emsembling various batches trained with different samples non-fraud values =).\n",
    "\n",
    "The next thing the method above does is compute covariate shift scores per feature, e.g. adversarial validation. The smaller the number, the more ideal the variable. The larger the value, the worse it is—the easier it is for our model to tell train samples from test samples. If there is too much shift, we can expect our model to fail at generalizing in the private lb. We should shoot for features that have <= 0.02 CVS. If you have a really good (high permutation importance auc, low permutation importance std) feature that also has a high CVS, try engineering, transforming, windsorizing, or otherwise degrading the variable until you get it within the cutoff threshold range.\n",
    "\n",
    "Moving on, the method above trains various overlapping folds. The folds are 90 days long, have a 15 day gap, and then use the next 20 days for validation. These windows are shifted +14 days each fold, until we get to the end of the dataset. As-Is, it doesn't matter what start date you set, because each day will still have 24hours. But if you generate features that are like holiday features, then start date becomes important.\n",
    "\n",
    "As each fold is trained, we calculate the permutation importance measure. There's a lot of talk about what that is so I won't go into it in detail here. Just know that it's not perfect. It's 10000x better than the stalk-feature-importance measures; but to get the true feature importance, we need to run a drop importance calculation. I haven't added it here because it slows down the execution a bit (re-trains the model for each feature); but it'll probably be in our best interest to switch to that anyway.\n",
    "\n",
    "Any feature that has a <0 permutation or drop importance should be discarded immediately. And any feature with a veri low importance, we can consider discarding recursively to test if it improves out model's score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found str ProductCD ... encoding!\n",
      "Found str card4 ... encoding!\n",
      "Found str card6 ... encoding!\n",
      "Found str P_emaildomain ... encoding!\n",
      "Found str R_emaildomain ... encoding!\n"
     ]
    }
   ],
   "source": [
    "data = traintr.append(testtr, sort=False)\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "features = [\n",
    "    'TransactionAmt',\n",
    "    'ProductCD',\n",
    "    'card1',\n",
    "    'card2',\n",
    "    'card3',\n",
    "    'card4',\n",
    "    'card5',\n",
    "    'card6',\n",
    "    'addr1',\n",
    "    'addr2',\n",
    "    'dist1',\n",
    "    'dist2',\n",
    "    'P_emaildomain',\n",
    "    'R_emaildomain',\n",
    "    'D3',\n",
    "    'D1',\n",
    "    'V286',\n",
    "    'V100',\n",
    "    'thour',\n",
    "]\n",
    "\n",
    "# LE:\n",
    "for col in features:\n",
    "    if data[col].dtype!='O': continue\n",
    "\n",
    "    print('Found str', col, '... encoding!')\n",
    "    mapper = {key:val for val,key in enumerate(data[col].unique())}\n",
    "    data[col] = data[col].map(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 1) [x] Negative Downsample (non-frauds)\n",
      "240109 total train samples!\n",
      "\n",
      "# 2) [x] Run adversarial validation (CVS) on features + record scores\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.539275\tvalid_1's auc: 0.531973\n",
      "Early stopping, best iteration is:\n",
      "[190]\ttraining's auc: 0.53914\tvalid_1's auc: 0.532023\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.508451\tvalid_1's auc: 0.507329\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.54245\tvalid_1's auc: 0.53612\n",
      "[400]\ttraining's auc: 0.544714\tvalid_1's auc: 0.537782\n",
      "Early stopping, best iteration is:\n",
      "[539]\ttraining's auc: 0.545423\tvalid_1's auc: 0.538444\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.545567\tvalid_1's auc: 0.539274\n",
      "[400]\ttraining's auc: 0.547487\tvalid_1's auc: 0.540255\n",
      "Early stopping, best iteration is:\n",
      "[393]\ttraining's auc: 0.547493\tvalid_1's auc: 0.54032\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.508112\tvalid_1's auc: 0.507865\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.50921\tvalid_1's auc: 0.509975\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.529436\tvalid_1's auc: 0.530299\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.516857\tvalid_1's auc: 0.515413\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's auc: 0.517284\tvalid_1's auc: 0.514037\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.504239\tvalid_1's auc: 0.503368\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.529081\tvalid_1's auc: 0.525204\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.514575\tvalid_1's auc: 0.514333\n",
      "Early stopping, best iteration is:\n",
      "[245]\ttraining's auc: 0.514979\tvalid_1's auc: 0.514493\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's auc: 0.520902\tvalid_1's auc: 0.519131\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.512274\tvalid_1's auc: 0.510784\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.533547\tvalid_1's auc: 0.529443\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's auc: 0.540896\tvalid_1's auc: 0.536932\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.500466\tvalid_1's auc: 0.501357\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.505298\tvalid_1's auc: 0.505573\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.514663\tvalid_1's auc: 0.5122\n",
      "\n",
      "#3) [x] Train on 50% overlapping folds on the trainset\n",
      "\n",
      "Fold 1 [0, 90] [105, 125]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.853475\tvalid_1's auc: 0.845681\n",
      "[400]\ttraining's auc: 0.878406\tvalid_1's auc: 0.85729\n",
      "[600]\ttraining's auc: 0.894477\tvalid_1's auc: 0.863022\n",
      "[800]\ttraining's auc: 0.906109\tvalid_1's auc: 0.866339\n",
      "[1000]\ttraining's auc: 0.915074\tvalid_1's auc: 0.868555\n",
      "Early stopping, best iteration is:\n",
      "[1140]\ttraining's auc: 0.920592\tvalid_1's auc: 0.86972\n",
      "baseline -  0.8697200229096337\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "Fold 2 [14, 104] [119, 139]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.851027\tvalid_1's auc: 0.830842\n",
      "[400]\ttraining's auc: 0.876138\tvalid_1's auc: 0.845372\n",
      "[600]\ttraining's auc: 0.892764\tvalid_1's auc: 0.85412\n",
      "[800]\ttraining's auc: 0.904624\tvalid_1's auc: 0.858465\n",
      "[1000]\ttraining's auc: 0.913867\tvalid_1's auc: 0.861512\n",
      "[1200]\ttraining's auc: 0.921265\tvalid_1's auc: 0.863398\n",
      "Early stopping, best iteration is:\n",
      "[1232]\ttraining's auc: 0.922354\tvalid_1's auc: 0.863796\n",
      "baseline -  0.8637959579324135\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "Fold 3 [28, 118] [133, 153]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.860098\tvalid_1's auc: 0.819383\n",
      "[400]\ttraining's auc: 0.884463\tvalid_1's auc: 0.834831\n",
      "[600]\ttraining's auc: 0.900485\tvalid_1's auc: 0.844143\n",
      "[800]\ttraining's auc: 0.910948\tvalid_1's auc: 0.847935\n",
      "[1000]\ttraining's auc: 0.919464\tvalid_1's auc: 0.851368\n",
      "[1200]\ttraining's auc: 0.926511\tvalid_1's auc: 0.853559\n",
      "[1400]\ttraining's auc: 0.932338\tvalid_1's auc: 0.855443\n",
      "[1600]\ttraining's auc: 0.937379\tvalid_1's auc: 0.857014\n",
      "[1800]\ttraining's auc: 0.94183\tvalid_1's auc: 0.857938\n",
      "Early stopping, best iteration is:\n",
      "[1852]\ttraining's auc: 0.942892\tvalid_1's auc: 0.858163\n",
      "baseline -  0.8581633046769573\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "Fold 4 [42, 132] [147, 167]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.86033\tvalid_1's auc: 0.816299\n",
      "[400]\ttraining's auc: 0.884716\tvalid_1's auc: 0.833265\n",
      "[600]\ttraining's auc: 0.900751\tvalid_1's auc: 0.842065\n",
      "[800]\ttraining's auc: 0.911546\tvalid_1's auc: 0.847066\n",
      "[1000]\ttraining's auc: 0.920309\tvalid_1's auc: 0.850138\n",
      "[1200]\ttraining's auc: 0.927394\tvalid_1's auc: 0.853229\n",
      "[1400]\ttraining's auc: 0.933192\tvalid_1's auc: 0.855092\n",
      "[1600]\ttraining's auc: 0.938211\tvalid_1's auc: 0.856571\n",
      "[1800]\ttraining's auc: 0.94271\tvalid_1's auc: 0.857753\n",
      "Early stopping, best iteration is:\n",
      "[1903]\ttraining's auc: 0.944839\tvalid_1's auc: 0.85826\n",
      "baseline -  0.858259900028247\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "Fold 5 [56, 146] [161, 181]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.861708\tvalid_1's auc: 0.825528\n",
      "[400]\ttraining's auc: 0.886315\tvalid_1's auc: 0.83733\n",
      "[600]\ttraining's auc: 0.901943\tvalid_1's auc: 0.843986\n",
      "[800]\ttraining's auc: 0.913041\tvalid_1's auc: 0.848905\n",
      "[1000]\ttraining's auc: 0.921164\tvalid_1's auc: 0.852611\n",
      "[1200]\ttraining's auc: 0.928093\tvalid_1's auc: 0.854899\n",
      "[1400]\ttraining's auc: 0.93379\tvalid_1's auc: 0.85664\n",
      "Early stopping, best iteration is:\n",
      "[1471]\ttraining's auc: 0.935668\tvalid_1's auc: 0.857333\n",
      "baseline -  0.8573325680523723\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "# 4) [x] Aggregate and save results\n"
     ]
    }
   ],
   "source": [
    "results = run_evaluation(\n",
    "    data,\n",
    "    features,\n",
    "    lgb_params,\n",
    "    downsample_seed=1773,\n",
    "    downsample_frac=0.2,\n",
    "    save_file_path='./report_test.json' # persist the results to a file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_auc': 0.8614543507199247,\n",
       " 'avg_iterations': 1519.6,\n",
       " 'avg_permutation_importance': {'D1': 0.032618919634878174,\n",
       "  'D3': 0.028840766128488737,\n",
       "  'P_emaildomain': 0.014683794128031492,\n",
       "  'ProductCD': 0.030510966011392537,\n",
       "  'R_emaildomain': 0.011916967840730642,\n",
       "  'TransactionAmt': 0.02936864784630633,\n",
       "  'V100': 0.0011276682543982063,\n",
       "  'V286': 0.0003692694287690923,\n",
       "  'addr1': 0.01368448981531727,\n",
       "  'addr2': 0.0005995180622625273,\n",
       "  'card1': 0.012887548343548372,\n",
       "  'card2': 0.012663633000950058,\n",
       "  'card3': 0.01350809771939796,\n",
       "  'card4': 0.0012496917270147234,\n",
       "  'card5': 0.004767352604428376,\n",
       "  'card6': 0.010942675920093348,\n",
       "  'dist1': 0.008940170807505976,\n",
       "  'dist2': 0.0009479430819911317,\n",
       "  'thour': 0.0038827460517223632},\n",
       " 'cvs': {'D1': 0.03693164823678563,\n",
       "  'D3': 0.02944289577506043,\n",
       "  'P_emaildomain': 0.019130652899962963,\n",
       "  'ProductCD': 0.007328661484768961,\n",
       "  'R_emaildomain': 0.010783549056648911,\n",
       "  'TransactionAmt': 0.0320228751326479,\n",
       "  'V100': 0.005573272653573547,\n",
       "  'V286': 0.0013574464828786104,\n",
       "  'addr1': 0.014036996308761873,\n",
       "  'addr2': 0.0033682134008644615,\n",
       "  'card1': 0.03844394445147836,\n",
       "  'card2': 0.040320274376888454,\n",
       "  'card3': 0.00786496670334802,\n",
       "  'card4': 0.009974517172619102,\n",
       "  'card5': 0.030299181672082454,\n",
       "  'card6': 0.015412653399625609,\n",
       "  'dist1': 0.025204422089920797,\n",
       "  'dist2': 0.014493223065347838,\n",
       "  'thour': 0.012199956832829595},\n",
       " 'downsample_frac': 0.2,\n",
       " 'downsample_seed': 1773,\n",
       " 'features': ['TransactionAmt',\n",
       "  'ProductCD',\n",
       "  'card1',\n",
       "  'card2',\n",
       "  'card3',\n",
       "  'card4',\n",
       "  'card5',\n",
       "  'card6',\n",
       "  'addr1',\n",
       "  'addr2',\n",
       "  'dist1',\n",
       "  'dist2',\n",
       "  'P_emaildomain',\n",
       "  'R_emaildomain',\n",
       "  'D3',\n",
       "  'D1',\n",
       "  'V286',\n",
       "  'V100',\n",
       "  'thour'],\n",
       " 'folds': [{'auc': 0.8697200229096337,\n",
       "   'fold_num': 0,\n",
       "   'iterations': 1140,\n",
       "   'permutation_importance': {'D1': 0.03525989702641874,\n",
       "    'D3': 0.032014754843684434,\n",
       "    'P_emaildomain': 0.015398787878206921,\n",
       "    'ProductCD': 0.007429596712357434,\n",
       "    'R_emaildomain': 0.019012462802386776,\n",
       "    'TransactionAmt': 0.03489776450218862,\n",
       "    'V100': 0.0010357198293610148,\n",
       "    'V286': 0.0006292948077486482,\n",
       "    'addr1': 0.008756597538531863,\n",
       "    'addr2': 0.001465754541144948,\n",
       "    'card1': 0.00928410269284008,\n",
       "    'card2': 0.008267284089757743,\n",
       "    'card3': 0.022377217933519433,\n",
       "    'card4': 0.00037771761457150443,\n",
       "    'card5': 0.0038363949976321088,\n",
       "    'card6': 0.012705808528736373,\n",
       "    'dist1': 0.005253096178062688,\n",
       "    'dist2': 0.00028611590923632857,\n",
       "    'thour': 0.004588401806325582},\n",
       "   'trn_range': [0, 90],\n",
       "   'val_range': [105, 125]},\n",
       "  {'auc': 0.8637959579324135,\n",
       "   'fold_num': 1,\n",
       "   'iterations': 1232,\n",
       "   'permutation_importance': {'D1': 0.03429144241902815,\n",
       "    'D3': 0.036828088369229794,\n",
       "    'P_emaildomain': 0.01494624812288714,\n",
       "    'ProductCD': 0.012101203959318907,\n",
       "    'R_emaildomain': 0.012993987870568269,\n",
       "    'TransactionAmt': 0.03720975508650448,\n",
       "    'V100': 0.0015181185393643437,\n",
       "    'V286': 0.000619318386572143,\n",
       "    'addr1': 0.009277834470669655,\n",
       "    'addr2': -0.0007560916623217118,\n",
       "    'card1': 0.012968910069120265,\n",
       "    'card2': 0.013771642795928574,\n",
       "    'card3': 0.014691266842007389,\n",
       "    'card4': 0.0017255674607140614,\n",
       "    'card5': 0.004334651389896638,\n",
       "    'card6': 0.01211725739845293,\n",
       "    'dist1': 0.010229575523368895,\n",
       "    'dist2': 0.0002722197372870827,\n",
       "    'thour': 0.004338074773131528},\n",
       "   'trn_range': [14, 104],\n",
       "   'val_range': [119, 139]},\n",
       "  {'auc': 0.8581633046769573,\n",
       "   'fold_num': 2,\n",
       "   'iterations': 1852,\n",
       "   'permutation_importance': {'D1': 0.034226487540551576,\n",
       "    'D3': 0.02794903729578646,\n",
       "    'P_emaildomain': 0.014755880795930598,\n",
       "    'ProductCD': 0.03655732141928503,\n",
       "    'R_emaildomain': 0.008020821625502661,\n",
       "    'TransactionAmt': 0.025937223580363877,\n",
       "    'V100': 0.001178915144518733,\n",
       "    'V286': 0.0001851764424506408,\n",
       "    'addr1': 0.015371370193039247,\n",
       "    'addr2': 0.00042607117514315007,\n",
       "    'card1': 0.016254738750716924,\n",
       "    'card2': 0.01581660127056883,\n",
       "    'card3': 0.007581989164530012,\n",
       "    'card4': 0.0009630397215092401,\n",
       "    'card5': 0.005706595391989477,\n",
       "    'card6': 0.010116272688427852,\n",
       "    'dist1': 0.009742612653396043,\n",
       "    'dist2': 0.0015544684031760791,\n",
       "    'thour': 0.004544563488535269},\n",
       "   'trn_range': [28, 118],\n",
       "   'val_range': [133, 153]},\n",
       "  {'auc': 0.858259900028247,\n",
       "   'fold_num': 3,\n",
       "   'iterations': 1903,\n",
       "   'permutation_importance': {'D1': 0.030982070081953195,\n",
       "    'D3': 0.022724317896347546,\n",
       "    'P_emaildomain': 0.017570912078961887,\n",
       "    'ProductCD': 0.046980027830594295,\n",
       "    'R_emaildomain': 0.008490281407356193,\n",
       "    'TransactionAmt': 0.025920507597147924,\n",
       "    'V100': 0.0015092657448396718,\n",
       "    'V286': 0.00018208346973314082,\n",
       "    'addr1': 0.017726894209438293,\n",
       "    'addr2': 0.00034640002027486183,\n",
       "    'card1': 0.012723013817655948,\n",
       "    'card2': 0.016961862528319993,\n",
       "    'card3': 0.009623773517065226,\n",
       "    'card4': 0.0016279351488562854,\n",
       "    'card5': 0.005616762238343731,\n",
       "    'card6': 0.007123458478182743,\n",
       "    'dist1': 0.00975135932304938,\n",
       "    'dist2': 0.0008925955255211226,\n",
       "    'thour': 0.002693104779450728},\n",
       "   'trn_range': [42, 132],\n",
       "   'val_range': [147, 167]},\n",
       "  {'auc': 0.8573325680523723,\n",
       "   'fold_num': 4,\n",
       "   'iterations': 1471,\n",
       "   'permutation_importance': {'D1': 0.02833470110643921,\n",
       "    'D3': 0.02468763223739545,\n",
       "    'P_emaildomain': 0.010747141764170909,\n",
       "    'ProductCD': 0.049486680135407024,\n",
       "    'R_emaildomain': 0.011067285497839308,\n",
       "    'TransactionAmt': 0.022877988465326737,\n",
       "    'V100': 0.00039632201390726873,\n",
       "    'V286': 0.00023047403734088867,\n",
       "    'addr1': 0.017289752664907287,\n",
       "    'addr2': 0.0015154562370713887,\n",
       "    'card1': 0.013206976387408642,\n",
       "    'card2': 0.008500774320175153,\n",
       "    'card3': 0.01326624113986774,\n",
       "    'card4': 0.0015541986894225257,\n",
       "    'card5': 0.004342359004279928,\n",
       "    'card6': 0.012650582506666841,\n",
       "    'dist1': 0.009724210359652874,\n",
       "    'dist2': 0.0017343158347350451,\n",
       "    'thour': 0.0032495854111687095},\n",
       "   'trn_range': [56, 146],\n",
       "   'val_range': [161, 181]}],\n",
       " 'params': {'bagging_seed': 260,\n",
       "  'boosting_type': 'gbdt',\n",
       "  'colsample_bytree': 0.7,\n",
       "  'data_random_seed': 262,\n",
       "  'drop_seed': 261,\n",
       "  'feature_fraction_seed': 259,\n",
       "  'learning_rate': 0.01,\n",
       "  'max_bin': 255,\n",
       "  'max_depth': -1,\n",
       "  'metric': 'auc',\n",
       "  'n_jobs': -1,\n",
       "  'num_leaves': 32,\n",
       "  'objective': 'binary',\n",
       "  'seed': 257,\n",
       "  'subsample': 0.7,\n",
       "  'subsample_freq': 1,\n",
       "  'tree_learner': 'serial',\n",
       "  'verbose': -1},\n",
       " 'std_auc': 0.004729250526718457,\n",
       " 'std_iterations': 311.9798711455596,\n",
       " 'std_permutation_importance': {'D1': 0.002584189369059892,\n",
       "  'D3': 0.005085787281169154,\n",
       "  'P_emaildomain': 0.0022098858136278465,\n",
       "  'ProductCD': 0.01754727947558808,\n",
       "  'R_emaildomain': 0.003978696173936586,\n",
       "  'TransactionAmt': 0.005618667041519832,\n",
       "  'V100': 0.00041085670247312164,\n",
       "  'V286': 0.00020896442307982298,\n",
       "  'addr1': 0.003895824752914304,\n",
       "  'addr2': 0.0008391827773062589,\n",
       "  'card1': 0.002211719575874822,\n",
       "  'card2': 0.0036414543201855696,\n",
       "  'card3': 0.005105417010650618,\n",
       "  'card4': 0.0005108343205534606,\n",
       "  'card5': 0.0007534196316860849,\n",
       "  'card6': 0.0021295345652667026,\n",
       "  'dist1': 0.0018533073996231475,\n",
       "  'dist2': 0.0006138298899008402,\n",
       "  'thour': 0.0007693393739634981}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Folds Used\n",
      "0.2 Neg DownSample Frac with 1773 Seed\n",
      "0.861 AVG AUC, 0.005 STD\n",
      "1519.6 AVG Rounds, 311.9798711455596 Rounds\n",
      "\n",
      "{'objective': 'binary', 'boosting_type': 'gbdt', 'metric': 'auc', 'n_jobs': -1, 'learning_rate': 0.01, 'num_leaves': 32, 'max_depth': -1, 'tree_learner': 'serial', 'colsample_bytree': 0.7, 'subsample_freq': 1, 'subsample': 0.7, 'max_bin': 255, 'verbose': -1, 'seed': 257, 'feature_fraction_seed': 259, 'bagging_seed': 260, 'drop_seed': 261, 'data_random_seed': 262}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/authman/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1706: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>adversarial</th>\n",
       "      <th>perm_import</th>\n",
       "      <th>perm_import_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D1</td>\n",
       "      <td>0.036932</td>\n",
       "      <td>0.032619</td>\n",
       "      <td>0.002584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProductCD</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.017547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionAmt</td>\n",
       "      <td>0.032023</td>\n",
       "      <td>0.029369</td>\n",
       "      <td>0.005619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D3</td>\n",
       "      <td>0.029443</td>\n",
       "      <td>0.028841</td>\n",
       "      <td>0.005086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>P_emaildomain</td>\n",
       "      <td>0.019131</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.002210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>addr1</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.013684</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>card3</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.005105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>card1</td>\n",
       "      <td>0.038444</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.002212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>card2</td>\n",
       "      <td>0.040320</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>0.003641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>R_emaildomain</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.011917</td>\n",
       "      <td>0.003979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>card6</td>\n",
       "      <td>0.015413</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.002130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dist1</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>0.001853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>card5</td>\n",
       "      <td>0.030299</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>thour</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>0.000769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>card4</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>V100</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dist2</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.000614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>addr2</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>V286</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  adversarial  perm_import  perm_import_std\n",
       "15              D1     0.036932     0.032619         0.002584\n",
       "1        ProductCD     0.007329     0.030511         0.017547\n",
       "0   TransactionAmt     0.032023     0.029369         0.005619\n",
       "14              D3     0.029443     0.028841         0.005086\n",
       "12   P_emaildomain     0.019131     0.014684         0.002210\n",
       "8            addr1     0.014037     0.013684         0.003896\n",
       "4            card3     0.007865     0.013508         0.005105\n",
       "2            card1     0.038444     0.012888         0.002212\n",
       "3            card2     0.040320     0.012664         0.003641\n",
       "13   R_emaildomain     0.010784     0.011917         0.003979\n",
       "7            card6     0.015413     0.010943         0.002130\n",
       "10           dist1     0.025204     0.008940         0.001853\n",
       "6            card5     0.030299     0.004767         0.000753\n",
       "18           thour     0.012200     0.003883         0.000769\n",
       "5            card4     0.009975     0.001250         0.000511\n",
       "17            V100     0.005573     0.001128         0.000411\n",
       "11           dist2     0.014493     0.000948         0.000614\n",
       "9            addr2     0.003368     0.000600         0.000839\n",
       "16            V286     0.001357     0.000369         0.000209"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b713cecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report, sns_features = display_report(results)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(x=\"perm_import\", y=\"feature\", data=sns_features, edgecolor=('white'), linewidth=2)#, palette=\"rocket\")\n",
    "plt.title('Permutation Importance (averaged/folds)', fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to look at regular gain or splits, the V columns would dominate the show. But here we accurately see they really aren't that important in the grand scheme of things. Notice how some of our variables have a very high CVS score, especially the high cardinality categorical variables, like `card2` for example. While the unique values present for card2 in train and test are very similar, their distributions shift a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(traintr.card2.unique())\n",
    "b = set(testtr.card2.unique())\n",
    "\n",
    "len(a-b), len(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321.0    48935\n",
       "111.0    45191\n",
       "555.0    41995\n",
       "490.0    38145\n",
       "583.0    21803\n",
       "170.0    18214\n",
       "194.0    16938\n",
       "545.0    16355\n",
       "360.0    15190\n",
       "514.0    14541\n",
       "174.0    11310\n",
       "512.0    10126\n",
       "408.0     8012\n",
       "361.0     7827\n",
       "100.0     7570\n",
       "Name: card2, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintr.card2.value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321.0    42796\n",
       "555.0    38409\n",
       "111.0    37346\n",
       "490.0    32351\n",
       "583.0    19700\n",
       "545.0    15539\n",
       "170.0    15197\n",
       "194.0    14573\n",
       "514.0    12684\n",
       "360.0    11342\n",
       "174.0    10096\n",
       "512.0     9758\n",
       "408.0     9148\n",
       "375.0     6684\n",
       "361.0     6593\n",
       "Name: card2, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtr.card2.value_counts().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much we can do there. I'm not advocating removing card2 as a variable; but I am advocating we use these results to draw our attention to possible issues. So for example, the appropriate thing to do here would be to attempt the removal of card2 and observe how it affects the model's mean and std AUC. That is the ultimate 'measure'.\n",
    "\n",
    "As we mentioned previously, any feature with a negative perm importance should be destroyed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still need to flesh out the submission pipeline. Today was really brutal at work... 9 hours straight of data analysis. I'll try to get this fleshed out ASAAPPPPPPP.\n",
    "\n",
    "Once we've decided on a subset of good engineered features to use, we must run the submission pipeline to submit. Submission pipeline is completely different from the feature validation pipeline. Rather, the sub pipeline builds many versions of a single model and merges them together. I wouldn't all this ensembling per se, since that'll happen is another, external script. But this is how we prepare our 'level-1' models for posting against Kaggle LB."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTES: \n",
    "\n",
    "Giba (different competition): I train pipeline was base on time splits so I have 3 stages:\n",
    "1) Train on a)Train and validate on b)Valid\n",
    "2) Train on b)Valid with no validation and using same number of rounds early stopped in 1)\n",
    "3) Train full train (a)+(b) using 2x number of rounds early stopped in 1)\n",
    "Result: blend using geometric average (1)^0.2 * (2)^0.2 * (3)^0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 13, 14],\n",
       " [13, 14, 15],\n",
       " [14, 15, 16],\n",
       " [15, 16, 17],\n",
       " [12, 13, 14, 15, 16, 17],\n",
       " [12, 13, 14, 15, 16, 17]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def submission(num_boost_rounds=0):\n",
    "    # We train using fixed num_boost_rounds\n",
    "    train_groups = []\n",
    "    for month_start in range(4):\n",
    "        # using 3x dif seeds each\n",
    "        months = [12 + month_start, 12 + month_start + 1, 12 + month_start + 2]\n",
    "        train_groups.append(months)\n",
    "        \n",
    "    # Then using double num_boost_rounds\n",
    "    train_groups += [[12,13,14,15,16,17]] #dif seed 2x\n",
    "    train_groups += [[12,13,14,15,16,17]] #dif seed 2x\n",
    "    return train_groups\n",
    "\n",
    "submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
