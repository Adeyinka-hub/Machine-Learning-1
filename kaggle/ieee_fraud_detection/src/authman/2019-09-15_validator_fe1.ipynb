{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have done this as our first step, but better late than never. This simple kernel demonstrates how to run local feature validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "import gc, warnings, json\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintr = pd.read_csv('input/train_transaction.csv.zip')\n",
    "trainid = pd.read_csv('input/train_identity.csv.zip')\n",
    "testtr  = pd.read_csv('input/test_transaction.csv.zip')\n",
    "testid  = pd.read_csv('input/test_identity.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-11-02 00:00:00')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each categorical variable, we'd like to experiment with\n",
    "# the count of appearances within that day's hour\n",
    "# This will only work if the distributions (counts) are similar in train + test\n",
    "\n",
    "START_DATE     = '2017-11-01'\n",
    "startdate      = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "traintr['tdt']    = traintr['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "traintr['thour']  = traintr.tdt.dt.hour\n",
    "traintr['tmonth'] = (traintr.tdt.dt.year-2017) * 12 + traintr.tdt.dt.month\n",
    "traintr['tweek'] = 52 * (traintr.tdt.dt.year-2017) + traintr.tdt.dt.weekofyear\n",
    "traintr['tdoy'] = 365 * (traintr.tdt.dt.year-2017) + traintr.tdt.dt.dayofyear\n",
    "traintr.tdoy -= traintr.tdoy.min() # 0-offset\n",
    "\n",
    "testtr['tdt']    = testtr['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "testtr['thour']  = testtr.tdt.dt.hour\n",
    "testtr['tmonth'] = (testtr.tdt.dt.year-2017) * 12 + testtr.tdt.dt.month\n",
    "testtr['tweek'] = 52 * (testtr.tdt.dt.year-2017) + testtr.tdt.dt.weekofyear\n",
    "testtr['tdoy'] = 365 * (testtr.tdt.dt.year-2017) + testtr.tdt.dt.dayofyear\n",
    "testtr.tdoy -= traintr.tdoy.min() # 0-offset\n",
    "\n",
    "traintr.tdt.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmonth\n",
       "11    130968\n",
       "12     92770\n",
       "13     96196\n",
       "14     90667\n",
       "15     87512\n",
       "16     86934\n",
       "17      5493\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintr.groupby('tmonth').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 181)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintr.tdoy.min(),traintr.tdoy.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2**8+1\n",
    "\n",
    "# TODO: Optimize\n",
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'boosting_type':'gbdt',\n",
    "    'metric':'auc',\n",
    "    'n_jobs':-1,\n",
    "    'learning_rate':0.01,\n",
    "    'num_leaves': 2**5, # 5-8\n",
    "    'max_depth':-1,\n",
    "    'tree_learner':'serial',\n",
    "    'colsample_bytree': 0.7, ######################\n",
    "    'subsample':0.7, ####################    \n",
    "    \n",
    "    'subsample_freq':1,\n",
    "\n",
    "    'max_bin':255,\n",
    "    'verbose':-1,\n",
    "    'seed': SEED,\n",
    "    'feature_fraction_seed': SEED + 2,\n",
    "    'bagging_seed': SEED + 3,\n",
    "    'drop_seed': SEED + 4,\n",
    "    'data_random_seed': SEED + 5,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_report(report):\n",
    "    print('{} Folds Used'.format(len(report['folds'])))\n",
    "    print('{} Neg DownSample Frac with {} Seed'.format(report['downsample_frac'], report['downsample_seed']))\n",
    "    print('{} AVG AUC, {} STD'.format(np.round(report['avg_auc'],3), np.round(report['std_auc'],3)))\n",
    "    print('{} AVG Train AUC, {} STD'.format(np.round(report['avg_train_auc'],3), np.round(report['std_train_auc'],3)))\n",
    "    print(np.abs(np.round(report['avg_train_auc'] - report['avg_auc'],3)), 'Train-Val AUC Drift')\n",
    "    print('{} AVG Rounds, {} STD'.format(report['avg_iterations'], report['std_iterations']), end='\\n\\n')\n",
    "\n",
    "    features = pd.DataFrame({\n",
    "        'feature': report['features'],\n",
    "        'adversarial': list(report['cvs'].values()),\n",
    "        'perm_import': list(report['avg_permutation_importance'].values()),\n",
    "        'perm_import_std': list(report['std_permutation_importance'].values()),\n",
    "    })\n",
    "    #if do_cvs\n",
    "    features.sort_values(['perm_import','adversarial'], ascending=False, inplace=True)\n",
    "    \n",
    "    sns_df = pd.DataFrame({\n",
    "        'feature' : sum([list(fold['permutation_importance'].keys()) for fold in results['folds']], []),\n",
    "        'perm_import': sum([list(fold['permutation_importance'].values()) for fold in results['folds']], []),\n",
    "        \n",
    "    })\n",
    "    sns_df.sort_values(['feature','perm_import'], ascending=False, inplace=True)\n",
    "    \n",
    "    print(report['params'])\n",
    "    return features, sns_df\n",
    "\n",
    "def compare_reports(report1, report2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(data, features, params, do_cvs=True, downsample_seed=None, downsample_frac=0.2, save_file_path=None):\n",
    "    # NOTE: data should contain, at minimal, all train + test samples,\n",
    "    # along with the isFraud column, for separation and scoring purposes.\n",
    "    gc.collect()\n",
    "    \n",
    "    # Run evaluation and store results in a report\n",
    "    # Steps:\n",
    "    # 1) [x] Negative Downsample non-frauds\n",
    "    # 2) [x] Run adversarial validation on features + record scores\n",
    "    # 3) [x] Train on 50% overlapping folds on the trainset\n",
    "    # 3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
    "    # 4) [x] Aggregate and save results\n",
    "    \n",
    "    report = {\n",
    "        'features': features,\n",
    "        'params': params,\n",
    "        'downsample_seed': downsample_seed,\n",
    "        'downsample_frac': downsample_frac,\n",
    "        'cvs': {},\n",
    "        'folds': [],\n",
    "        'avg_permutation_importance': {},\n",
    "        'std_permutation_importance': {},\n",
    "    }\n",
    "    \n",
    "    ######################\n",
    "    print('\\n# 1) [x] Negative Downsample (non-frauds)')\n",
    "    if downsample_seed is None:\n",
    "        selection = data.copy()\n",
    "    else:\n",
    "        np.random.seed(downsample_seed)\n",
    "        normies = data[data.isFraud==0].index.values\n",
    "        normies = np.random.choice(\n",
    "            normies,\n",
    "            int(data.shape[0]*downsample_frac),\n",
    "            replace=False\n",
    "        )\n",
    "        selection = data[data.index.isin(\n",
    "            # All fruds and a number of normies\n",
    "            np.concatenate([normies, data[data.isFraud==1].index.values])\n",
    "        )].copy()\n",
    "    print(selection.shape[0], 'total train samples!')\n",
    "    \n",
    "    if selection.shape[0] > data.isFraud.isna().sum():\n",
    "        # If we have more train samples than test samples, use all test samples\n",
    "        selection_test = data[data.isFraud.isna()]\n",
    "    else:\n",
    "        # Use a balanced set of test samples \n",
    "        selection_test = np.random.choice(\n",
    "            data[data.isFraud.isna()].index.values,\n",
    "            selection.shape[0],\n",
    "            replace=False\n",
    "        )\n",
    "        selection_test = data[data.index.isin(selection_test)]\n",
    "\n",
    "    ######################\n",
    "    if do_cvs:\n",
    "        print('\\n# 2) [x] Run adversarial validation (CVS) on features + record scores')\n",
    "        # Build CVS dataset\n",
    "        cvsdata = selection.append(selection_test, sort=False)\n",
    "        cvsdata.reset_index(inplace=True)\n",
    "        cvsdata['which_set'] = (np.arange(cvsdata.shape[0]) >= selection.shape[0]).astype(np.uint8)\n",
    "        cvsdata = cvsdata.sample(frac=1).reset_index(drop=True) # Shuffle the thing\n",
    "        trn_cvs = cvsdata.index < (cvsdata.shape[0] // 2)\n",
    "        for col in features:\n",
    "            trn_lgb = lgb.Dataset(cvsdata[trn_cvs][[col]], label=cvsdata[trn_cvs].which_set)\n",
    "            val_lgb = lgb.Dataset(cvsdata[~trn_cvs][[col]], label=cvsdata[~trn_cvs].which_set)\n",
    "            clf = lgb.train(\n",
    "                params,\n",
    "                trn_lgb,\n",
    "                valid_sets = [trn_lgb, val_lgb],\n",
    "                verbose_eval = 200,\n",
    "                early_stopping_rounds = 25,\n",
    "                num_boost_round = 80000,\n",
    "            )\n",
    "            report['cvs'][col] = clf.best_score['valid_1']['auc'] - 0.5 # 0.5 = 0, best score\n",
    "        del cvsdata, trn_lgb, val_lgb, trn_cvs; gc.collect()\n",
    "    \n",
    "    ######################\n",
    "    print('\\n#3) [x] Train on 50% overlapping folds on the trainset')\n",
    "    for fold_, i in enumerate(range(0,57,14)):\n",
    "        gc.collect()\n",
    "        fold = {\n",
    "            'fold_num': fold_,\n",
    "            'trn_range': [i,i+90],\n",
    "            'val_range': [i+90+15,i+90+15+20],\n",
    "        }\n",
    "        print('\\nFold', fold_+1, '— Train', fold['trn_range'], '— Test', fold['val_range'])\n",
    "        \n",
    "        trn = selection[selection.tdoy.between(i, 90+i)]\n",
    "        val = selection[selection.tdoy.between(90+i+15, 90+i+15+20)].copy()\n",
    "        trn_lgb = lgb.Dataset(trn[features], label=trn.isFraud)\n",
    "        val_lgb = lgb.Dataset(val[features], label=val.isFraud)\n",
    "        clf = lgb.train(\n",
    "            params,\n",
    "            trn_lgb,\n",
    "            valid_sets = [trn_lgb, val_lgb],\n",
    "            verbose_eval = 200,\n",
    "            early_stopping_rounds = 25,\n",
    "            num_boost_round = 80000,\n",
    "            #categorical_feature=[]\n",
    "        )\n",
    "        baseline = clf.best_score['valid_1']['auc']\n",
    "        fold['auc'] = baseline\n",
    "        fold['train_auc'] = clf.best_score['training']['auc']\n",
    "        fold['iterations'] = clf.best_iteration\n",
    "        print('baseline - ', baseline)\n",
    "        \n",
    "        ######################\n",
    "        # TODO: Repalce with Drop importance\n",
    "        print('\\n#3b) [x] Perform permutation importance (soon to be drop importance) each fold')\n",
    "        perm = {}\n",
    "        for col in features:\n",
    "            backup = val[col].values.copy()\n",
    "            val[col] = np.random.permutation(val[col].values)\n",
    "            \n",
    "            y_true = clf.predict(val[features])\n",
    "            perm[col] = baseline - roc_auc_score(val.isFraud, y_true)\n",
    "            val[col] = backup\n",
    "        fold['permutation_importance'] = perm\n",
    "        report['folds'].append(fold)\n",
    "    \n",
    "    ######################\n",
    "    print('\\n# 4) [x] Aggregate and save results')\n",
    "    aucs = [fold['auc'] for fold in report['folds']]\n",
    "    report['avg_auc'] = np.mean(aucs)\n",
    "    report['std_auc'] = np.std(aucs)\n",
    "    \n",
    "    aucs = [fold['train_auc'] for fold in report['folds']]\n",
    "    report['avg_train_auc'] = np.mean(aucs)\n",
    "    report['std_train_auc'] = np.std(aucs)\n",
    "\n",
    "    iterations = [fold['iterations'] for fold in report['folds']]\n",
    "    report['avg_iterations'] = np.mean(iterations)\n",
    "    report['std_iterations'] = np.std(iterations)\n",
    "\n",
    "    for feature in features:\n",
    "        pi = [fold['permutation_importance'][feature] for fold in report['folds']]\n",
    "        report['avg_permutation_importance'][feature] = np.mean(pi)\n",
    "        report['std_permutation_importance'][feature] = np.std(pi)\n",
    "\n",
    "    if save_file_path is not None:\n",
    "        with open(save_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    gc.collect()\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above does a few things. First, it downsamples the negative values. This was reported by a number of top-50 people on the forums, and gold medal winners in previous competitions have used this technique as well. The idea being that fraud is unique and it shouldn't matter much which non-frauds we train against, since the frauds are a distinct class and should have some level of 'difference' about them. Think outliers. This would also be a good time to mention that when we train for _submission_, we can add diversity to our models by emsembling various batches trained with different samples non-fraud values =).\n",
    "\n",
    "The next thing the method above does is compute covariate shift scores per feature, e.g. adversarial validation. The smaller the number, the more ideal the variable. The larger the value, the worse it is—the easier it is for our model to tell train samples from test samples. If there is too much shift, we can expect our model to fail at generalizing in the private lb. We should shoot for features that have <= 0.02 CVS. If you have a really good (high permutation importance auc, low permutation importance std) feature that also has a high CVS, try engineering, transforming, windsorizing, or otherwise degrading the variable until you get it within the cutoff threshold range.\n",
    "\n",
    "Moving on, the method above trains various overlapping folds. The folds are 90 days long, have a 15 day gap, and then use the next 20 days for validation. These windows are shifted +14 days each fold, until we get to the end of the dataset. As-Is, it doesn't matter what start date you set, because each day will still have 24hours. But if you generate features that are like holiday features, then start date becomes important.\n",
    "\n",
    "As each fold is trained, we calculate the permutation importance measure. There's a lot of talk about what that is so I won't go into it in detail here (https://explained.ai/rf-importance/ https://arxiv.org/abs/1706.03825). Just know that it's not perfect. It's 10000x better than the stalk-feature-importance measures; but to get the true feature importance, we need to run a drop importance calculation. I haven't added it here because it slows down the execution a bit (re-trains the model for each feature); but it'll probably be in our best interest to switch to that anyway.\n",
    "\n",
    "Any feature that has a <0 permutation or drop importance should be discarded immediately. And any feature with a veri low importance, we can consider discarding recursively to test if it improves out model's score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train + test\n",
    "gc.collect()\n",
    "data = traintr.append(testtr, sort=False)\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling card2 7179 NAs filled using {ProductCD,card1}\n",
      "Filling card3 4421 NAs filled using {ProductCD,card1}\n",
      "Filling card4 4460 NAs filled using {ProductCD,card1}\n",
      "Filling card5 5868 NAs filled using {ProductCD,card1}\n",
      "Filling card6 4410 NAs filled using {ProductCD,card1}\n"
     ]
    }
   ],
   "source": [
    "# # Fill NA card data inasmuch as possible:\n",
    "# fill_cols = ['card2','card3','card4','card5','card6']\n",
    "# for col in fill_cols:\n",
    "#     card12 = data[~data.card2.isna()][['card1',col]].groupby(['card1',col]).size().reset_index()\n",
    "\n",
    "#     card12.rename(columns={0:'card1_cnt'}, inplace=True)\n",
    "#     card12_cnts = card12[card12.card1_cnt>2].copy() # threshold of instances of pairings needed before we accept it\n",
    "#     card12 = card12.groupby('card1').size().reset_index()\n",
    "#     card12.rename(columns={0:col+'_cnt'}, inplace=True)\n",
    "#     card12 = card12[card12[col+'_cnt']==1] # filter out any card where we see more than one card2 per card1\n",
    "#     card12 = card12.merge(card12_cnts, how='inner', on='card1')[['card1',col]]\n",
    "\n",
    "#     mapping = {card1:col_val for card1,col_val in card12.values}\n",
    "#     nas = data[col].isna()\n",
    "#     data.loc[nas, col] = data[nas].card1.map(mapping)\n",
    "#     print('Filling', col, nas.sum(), 'initial nas.', data[col].isna().sum(), 'ending nas')\n",
    "\n",
    "# Fill NA card data inasmuch as possible:\n",
    "fill_cols = ['card2','card3','card4','card5','card6']\n",
    "for col in fill_cols:\n",
    "    # We use both ProductCD+card1\n",
    "    card12 = data[~data.card2.isna()][['ProductCD','card1',col]].groupby(['ProductCD','card1',col]).size().reset_index()\n",
    "\n",
    "    card12.rename(columns={0:'card1_cnt'}, inplace=True)\n",
    "    card12_cnts = card12[card12.card1_cnt>=2].copy() # threshold of instances of pairings needed before we accept it\n",
    "    card12 = card12.groupby(['ProductCD','card1']).size().reset_index()\n",
    "    card12.rename(columns={0:col+'_cnt'}, inplace=True)\n",
    "    card12 = card12[card12[col+'_cnt']==1] # filter out any card where we see more than one card2 per card1\n",
    "    card12 = card12.merge(card12_cnts, how='inner', on=['ProductCD','card1'])[['ProductCD','card1',col]]\n",
    "\n",
    "    initial_nans = data[col].isna().sum()\n",
    "    for pcd in data.ProductCD.unique():\n",
    "        mapping = {card1:col_val for card1,col_val in card12[card12.ProductCD==pcd][['card1',col]].values}\n",
    "        nas = (data.ProductCD==pcd) & data[col].isna()\n",
    "        data.loc[nas, col] = data[nas].card1.map(mapping)\n",
    "    print('Filling', col, initial_nans - data[col].isna().sum(), 'NAs filled using {ProductCD,card1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build out additional ID columns\n",
    "stringy = lambda x: x.astype(str) + ';'\n",
    "\n",
    "# Card Stuff\n",
    "# card1 = UID\n",
    "# card2 = issuer bank name\n",
    "# card3 = country issued in\n",
    "# card4 = visa/master = card name\n",
    "# card5 = ?sourcesoftware?\n",
    "# card6 = creditdebit = card category of transaction\n",
    "data['card12'] = stringy(data.card1) + stringy(data.card2)\n",
    "data['card1235'] = stringy(data.card12) + stringy(data.card3) + stringy(data.card5)\n",
    "data['card1235addr1'] = stringy(data.card1235) + stringy(data.addr1)\n",
    "data['card34'] = stringy(data.card3) + stringy(data.card4)\n",
    "data['card13'] = stringy(data.card1) + stringy(data.card3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FE Sole columns\n",
    "data['dist'] = data.dist1\n",
    "data.loc[~data.dist2.isna(), 'dist'] = data[~data.dist2.isna()].dist2\n",
    "\n",
    "data['TransactionAmtCents'] = data.TransactionAmt - np.floor(data.TransactionAmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FE Email Stuff\n",
    "country_map = {\n",
    "    'com':'us', 'net':'us', 'edu':'us', 'gmail':'us', \n",
    "    'mx': 'mx', 'es':'es', 'de':'de', 'fr':'fr',\n",
    "    'uk':'uk', 'jp':'jp'\n",
    "}\n",
    "domain = lambda x: x.split('.')[0]\n",
    "pemail_country = lambda x: x.split('.')[-1]\n",
    "data['pemail_domain']  = data.P_emaildomain.astype(str).apply(domain)\n",
    "data['pemail_ext']     = data.P_emaildomain.astype(str).apply(pemail_country).map(country_map)\n",
    "data['remail_domain']  = data.R_emaildomain.astype(str).apply(domain)\n",
    "data['remail_ext']     = data.R_emaildomain.astype(str).apply(pemail_country).map(country_map)\n",
    "data['p_and_r_email']  = data.P_emaildomain.astype(str) + ' ' + data.R_emaildomain.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found str M1 ... encoding!\n",
      "Found str M2 ... encoding!\n",
      "Found str M3 ... encoding!\n",
      "Found str M4 ... encoding!\n",
      "Found str M5 ... encoding!\n",
      "Found str M6 ... encoding!\n",
      "Found str M7 ... encoding!\n",
      "Found str M8 ... encoding!\n",
      "Found str M9 ... encoding!\n"
     ]
    }
   ],
   "source": [
    "# Features to Label Encode\n",
    "# Makes join calculations more efficient.\n",
    "features = [\n",
    "#     'TransactionAmt',\n",
    "    'ProductCD',\n",
    "    'card1',\n",
    "    'card2',\n",
    "    'card3',\n",
    "    'card4',\n",
    "    'card5',\n",
    "    'card6',\n",
    "    'card12', 'card1235', 'card1235addr1', 'card34', \n",
    "    'card13',\n",
    "    \n",
    "#     'addr1',\n",
    "#     'addr2',\n",
    "#     'dist1',\n",
    "#     'dist2',\n",
    "#     'P_emaildomain',\n",
    "#     'R_emaildomain',\n",
    "#     'D3',\n",
    "#     'D1',\n",
    "#     'V286',\n",
    "#     'V100',\n",
    "#     'thour',\n",
    "    'pemail_domain', 'remail_domain', 'pemail_ext', 'remail_ext', 'p_and_r_email',\n",
    "    \n",
    "    'M1','M2','M3','M4','M5','M6','M7','M8','M9',\n",
    "]\n",
    "\n",
    "# LE:\n",
    "for col in features:\n",
    "    if data[col].dtype!='O': continue\n",
    "\n",
    "    # TODO whats happening to my nans?\n",
    "    print('Found str', col, '... encoding!')\n",
    "    mapper = {key:val for val,key in enumerate(data[~data[col].isna()][col].unique())}\n",
    "    data[col] = data[col].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:04<00:00, 10.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# TransactionAmt Feature Engineering\n",
    "# Using ProductCD + UIDs\n",
    "uids = [\n",
    "    'card1','card2','card3','card4','card5','card6',\n",
    "    'card12', 'card1235', 'card1235addr1', 'card34',\n",
    "    'addr1', 'card13',\n",
    "]\n",
    "\n",
    "for uid in tqdm(uids):\n",
    "    new_colA = '{}_grp__TransactionAmt_mean'.format(uid)\n",
    "    new_colB = 'me2__' + new_colA\n",
    "    means = data.groupby(['ProductCD',uid]).TransactionAmt.mean().reset_index()\n",
    "    means.rename(columns={'TransactionAmt':new_colA}, inplace=True)\n",
    "    data = data.merge(means, how='left', on=['ProductCD',uid])\n",
    "    data[new_colB] = data.TransactionAmt - data[new_colA]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:13<00:00, 11.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Do I want to do this? Or do I just want trxamtcents VS productcd??\n",
    "for uid in tqdm(uids):\n",
    "    new_colA = '{}_grp__TransactionAmtCents_mean'.format(uid)\n",
    "    new_colB = 'me2__' + new_colA\n",
    "    means = data.groupby(['ProductCD',uid]).TransactionAmtCents.mean().reset_index()\n",
    "    means.rename(columns={'TransactionAmtCents':new_colA}, inplace=True)\n",
    "    data = data.merge(means, how='left', on=['ProductCD',uid])\n",
    "    data[new_colB] = data.TransactionAmtCents - data[new_colA]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:21<00:00, 11.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# TransactionAmt Feature Engineering\n",
    "# Using ProductCD + UIDs\n",
    "uids = [\n",
    "    'card1','card2','card3','card4','card5','card6',\n",
    "    'card12', 'card1235', 'card1235addr1', 'card34',\n",
    "    'addr1', 'card13',\n",
    "]\n",
    "\n",
    "for uid in tqdm(uids):\n",
    "    new_colA = '{}_grp__dist_mean'.format(uid)\n",
    "    new_colB = 'me2__' + new_colA\n",
    "    means = data.groupby(['ProductCD',uid]).dist.mean().reset_index()\n",
    "    means.rename(columns={'dist':new_colA}, inplace=True)\n",
    "    data = data.merge(means, how='left', on=['ProductCD',uid])\n",
    "    data[new_colB] = data.dist - data[new_colA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# One Offs\n",
    "uids = [\n",
    "    'p_and_r_email', # should be CE'd\n",
    "]\n",
    "\n",
    "for uid in tqdm(uids):\n",
    "    new_colA = '{}_grp__D3_mean'.format(uid)\n",
    "    new_colB = 'me2__' + new_colA\n",
    "    means = data.groupby(['ProductCD',uid]).D3.mean().reset_index()\n",
    "    means.rename(columns={'D3':new_colA}, inplace=True)\n",
    "    data = data.merge(means, how='left', on=['ProductCD',uid])\n",
    "    data[new_colB] = data.D3 - data[new_colA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# One Offs\n",
    "uids = [\n",
    "    'remail_domain',\n",
    "]\n",
    "\n",
    "for uid in tqdm(uids):\n",
    "    new_colA = '{}_grp__D2_mean'.format(uid)\n",
    "    new_colB = 'me2__' + new_colA\n",
    "    means = data.groupby(['ProductCD',uid]).D2.mean().reset_index()\n",
    "    means.rename(columns={'D2':new_colA}, inplace=True)\n",
    "    data = data.merge(means, how='left', on=['ProductCD',uid])\n",
    "    data[new_colB] = data.D2 - data[new_colA]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./all_data_persist.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actually Used Features\n",
    "features = [\n",
    "    # Short list\n",
    "    'TransactionAmtCents',\n",
    "    'ProductCD',\n",
    "    'dist',\n",
    "    \n",
    "    'addr1', 'D8', 'D9',  # Questionable\n",
    "    \n",
    "    'addr1_grp__TransactionAmtCents_mean',\n",
    "    'me2__card3_grp__TransactionAmt_mean',\n",
    "    'me2__card34_grp__TransactionAmtCents_mean',\n",
    "    'me2__addr1_grp__TransactionAmtCents_mean',\n",
    "    'pemail_domain',\n",
    "    'remail_ext', 'p_and_r_email',\n",
    "    'M4','M5','M6',\n",
    "    \n",
    "    # High scoring suspicious columns (either cvs or permstd)\n",
    "    'card34_grp__TransactionAmt_mean',\n",
    "    'card2_grp__dist_mean',\n",
    "    'me2__p_and_r_email_grp__D3_mean',\n",
    "    'p_and_r_email_grp__D3_mean',\n",
    "    'remail_domain_grp__D2_mean',\n",
    "    # 'C11',\n",
    "    \n",
    "    # dropped\n",
    "    # 'card2_grp__TransactionAmt_mean',\n",
    "    \n",
    "    # testing\n",
    "    'V232',\n",
    "    \n",
    "    # C columns...\n",
    "    'C1','C2','C5','C6','C8','C9','C13','C14',\n",
    "]\n",
    "\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 1) [x] Negative Downsample (non-frauds)\n",
      "240109 total train samples!\n",
      "\n",
      "# 2) [x] Run adversarial validation (CVS) on features + record scores\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.529685\tvalid_1's auc: 0.526553\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.509306\tvalid_1's auc: 0.509348\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's auc: 0.532577\tvalid_1's auc: 0.530105\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.515257\tvalid_1's auc: 0.513235\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's auc: 0.514326\tvalid_1's auc: 0.512743\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.505908\tvalid_1's auc: 0.505534\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.520229\tvalid_1's auc: 0.5187\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.540093\tvalid_1's auc: 0.535051\n",
      "[400]\ttraining's auc: 0.54198\tvalid_1's auc: 0.536161\n",
      "Early stopping, best iteration is:\n",
      "[564]\ttraining's auc: 0.542689\tvalid_1's auc: 0.536731\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's auc: 0.535335\tvalid_1's auc: 0.530456\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.532662\tvalid_1's auc: 0.527854\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttraining's auc: 0.53339\tvalid_1's auc: 0.528226\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.519784\tvalid_1's auc: 0.519043\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.510265\tvalid_1's auc: 0.510709\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's auc: 0.528786\tvalid_1's auc: 0.525906\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.510515\tvalid_1's auc: 0.511672\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.508647\tvalid_1's auc: 0.508772\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.508532\tvalid_1's auc: 0.507605\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's auc: 0.521836\tvalid_1's auc: 0.518067\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.550279\tvalid_1's auc: 0.544746\n",
      "[400]\ttraining's auc: 0.55275\tvalid_1's auc: 0.546185\n",
      "Early stopping, best iteration is:\n",
      "[428]\ttraining's auc: 0.552896\tvalid_1's auc: 0.546235\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.545003\tvalid_1's auc: 0.539515\n",
      "Early stopping, best iteration is:\n",
      "[194]\ttraining's auc: 0.544751\tvalid_1's auc: 0.53959\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's auc: 0.529213\tvalid_1's auc: 0.527596\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.517048\tvalid_1's auc: 0.516053\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.514906\tvalid_1's auc: 0.513839\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.520552\tvalid_1's auc: 0.519059\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's auc: 0.518972\tvalid_1's auc: 0.517806\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.512782\tvalid_1's auc: 0.511284\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's auc: 0.513371\tvalid_1's auc: 0.513409\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.518632\tvalid_1's auc: 0.517426\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.520783\tvalid_1's auc: 0.518573\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's auc: 0.525213\tvalid_1's auc: 0.524394\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's auc: 0.51491\tvalid_1's auc: 0.513961\n",
      "\n",
      "#3) [x] Train on 50% overlapping folds on the trainset\n",
      "\n",
      "Fold 1 — Train [0, 90] — Test [105, 125]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.898428\tvalid_1's auc: 0.883773\n",
      "[400]\ttraining's auc: 0.918867\tvalid_1's auc: 0.894174\n",
      "[600]\ttraining's auc: 0.933018\tvalid_1's auc: 0.901101\n",
      "[800]\ttraining's auc: 0.94212\tvalid_1's auc: 0.904072\n",
      "[1000]\ttraining's auc: 0.949376\tvalid_1's auc: 0.906252\n",
      "[1200]\ttraining's auc: 0.955016\tvalid_1's auc: 0.907696\n",
      "[1400]\ttraining's auc: 0.959599\tvalid_1's auc: 0.908778\n",
      "[1600]\ttraining's auc: 0.963418\tvalid_1's auc: 0.909654\n",
      "[1800]\ttraining's auc: 0.966728\tvalid_1's auc: 0.910624\n",
      "Early stopping, best iteration is:\n",
      "[1824]\ttraining's auc: 0.967154\tvalid_1's auc: 0.910706\n",
      "baseline -  0.9107055290863204\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "Fold 2 — Train [14, 104] — Test [119, 139]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.897635\tvalid_1's auc: 0.869552\n",
      "[400]\ttraining's auc: 0.917843\tvalid_1's auc: 0.883568\n",
      "[600]\ttraining's auc: 0.931699\tvalid_1's auc: 0.893613\n",
      "[800]\ttraining's auc: 0.941537\tvalid_1's auc: 0.899747\n",
      "[1000]\ttraining's auc: 0.948616\tvalid_1's auc: 0.903797\n",
      "[1200]\ttraining's auc: 0.9544\tvalid_1's auc: 0.906603\n",
      "[1400]\ttraining's auc: 0.95897\tvalid_1's auc: 0.908391\n",
      "[1600]\ttraining's auc: 0.963058\tvalid_1's auc: 0.909857\n",
      "Early stopping, best iteration is:\n",
      "[1639]\ttraining's auc: 0.963775\tvalid_1's auc: 0.910108\n",
      "baseline -  0.9101077654440777\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "Fold 3 — Train [28, 118] — Test [133, 153]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.900024\tvalid_1's auc: 0.850111\n",
      "[400]\ttraining's auc: 0.919938\tvalid_1's auc: 0.86736\n",
      "[600]\ttraining's auc: 0.934235\tvalid_1's auc: 0.87921\n",
      "[800]\ttraining's auc: 0.943478\tvalid_1's auc: 0.885619\n",
      "[1000]\ttraining's auc: 0.950329\tvalid_1's auc: 0.890456\n",
      "[1200]\ttraining's auc: 0.955937\tvalid_1's auc: 0.893418\n",
      "[1400]\ttraining's auc: 0.960465\tvalid_1's auc: 0.896214\n",
      "[1600]\ttraining's auc: 0.964447\tvalid_1's auc: 0.89788\n",
      "[1800]\ttraining's auc: 0.9675\tvalid_1's auc: 0.898813\n",
      "[2000]\ttraining's auc: 0.97051\tvalid_1's auc: 0.899696\n",
      "Early stopping, best iteration is:\n",
      "[2149]\ttraining's auc: 0.972418\tvalid_1's auc: 0.900431\n",
      "baseline -  0.9004310352109022\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "Fold 4 — Train [42, 132] — Test [147, 167]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.899255\tvalid_1's auc: 0.862781\n",
      "[400]\ttraining's auc: 0.920288\tvalid_1's auc: 0.878356\n",
      "[600]\ttraining's auc: 0.934913\tvalid_1's auc: 0.88832\n",
      "[800]\ttraining's auc: 0.943929\tvalid_1's auc: 0.892999\n",
      "[1000]\ttraining's auc: 0.950671\tvalid_1's auc: 0.896029\n",
      "[1200]\ttraining's auc: 0.956213\tvalid_1's auc: 0.898393\n",
      "[1400]\ttraining's auc: 0.960919\tvalid_1's auc: 0.900297\n",
      "[1600]\ttraining's auc: 0.964662\tvalid_1's auc: 0.902058\n",
      "[1800]\ttraining's auc: 0.967894\tvalid_1's auc: 0.903251\n",
      "[2000]\ttraining's auc: 0.970778\tvalid_1's auc: 0.904442\n",
      "Early stopping, best iteration is:\n",
      "[2122]\ttraining's auc: 0.972204\tvalid_1's auc: 0.90501\n",
      "baseline -  0.9050103713221094\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5 — Train [56, 146] — Test [161, 181]\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's auc: 0.899762\tvalid_1's auc: 0.878133\n",
      "[400]\ttraining's auc: 0.92217\tvalid_1's auc: 0.890681\n",
      "[600]\ttraining's auc: 0.936175\tvalid_1's auc: 0.89765\n",
      "[800]\ttraining's auc: 0.945201\tvalid_1's auc: 0.901661\n",
      "[1000]\ttraining's auc: 0.952017\tvalid_1's auc: 0.904786\n",
      "[1200]\ttraining's auc: 0.957505\tvalid_1's auc: 0.906988\n",
      "[1400]\ttraining's auc: 0.961737\tvalid_1's auc: 0.90896\n",
      "[1600]\ttraining's auc: 0.965473\tvalid_1's auc: 0.910377\n",
      "Early stopping, best iteration is:\n",
      "[1697]\ttraining's auc: 0.967057\tvalid_1's auc: 0.911097\n",
      "baseline -  0.9110965294822831\n",
      "\n",
      "#3b) [x] Perform permutation importance (soon to be drop importance) each fold\n",
      "\n",
      "# 4) [x] Aggregate and save results\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "results = run_evaluation(\n",
    "    data,\n",
    "    features,\n",
    "    lgb_params,\n",
    "    downsample_seed=1773+1,\n",
    "    downsample_frac=0.2,\n",
    "    save_file_path='./report_test.json' # persist the results to a file\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5 Folds Used\n",
    "0.2 Neg DownSample Frac with 1774 Seed\n",
    "0.803 AVG AUC, 0.006 STD\n",
    "0.868 AVG Train AUC, 0.029 STD\n",
    "942.4 AVG Rounds, 724.6181339160647 STD\n",
    "\n",
    "--\n",
    "# 'ProductCD', 'me2__card3_grp__TransactionAmt_mean','card34_grp__TransactionAmt_mean','dist',\n",
    "5 Folds Used\n",
    "0.2 Neg DownSample Frac with 1774 Seed\n",
    "0.77 AVG AUC, 0.008 STD\n",
    "0.78 AVG Train AUC, 0.019 STD\n",
    "167.2 AVG Rounds, 204.62101553848274 STD\n",
    "\n",
    "--\n",
    "# Above + 'card2_grp__dist_mean','card34_grp__dist_mean',\n",
    "5 Folds Used\n",
    "0.2 Neg DownSample Frac with 1774 Seed\n",
    "0.785 AVG AUC, 0.014 STD\n",
    "0.803 AVG Train AUC, 0.038 STD\n",
    "202.2 AVG Rounds, 305.7589900558935 STD\n",
    "--\n",
    "Now @ 'TransactionAmtCents',  'ProductCD',  'dist',  'card2_grp__dist_mean',  'card34_grp__TransactionAmt_mean',  'addr1_grp__TransactionAmtCents_mean',  'me2__card3_grp__TransactionAmt_mean',  'me2__card34_grp__TransactionAmtCents_mean',  'me2__addr1_grp__TransactionAmtCents_mean'\n",
    "0.801 AVG AUC, 0.013 STD\n",
    "0.845 AVG Train AUC, 0.039 STD\n",
    "0.044 Train-Val AUC Drift\n",
    "458.0 AVG Rounds, 499.27827911896986 STD\n",
    "--\n",
    "Added 'pemail_domain','remail_ext', 'p_and_r_email','M4','M5','M6',\n",
    "0.833 AVG AUC, 0.016 STD\n",
    "0.89 AVG Train AUC, 0.056 STD\n",
    "0.057 Train-Val AUC Drift\n",
    "1167.4 AVG Rounds, 958.0953188488085 STD\n",
    "--\n",
    "D3 aggregates:\n",
    "0.859 AVG AUC, 0.002 STD\n",
    "0.928 AVG Train AUC, 0.007 STD\n",
    "0.069 Train-Val AUC Drift\n",
    "1515.8 AVG Rounds, 255.70248336690042 STD\n",
    "\n",
    "removed: card2_grp__dist_mean\n",
    "0.839 AVG AUC, 0.012 STD\n",
    "0.892 AVG Train AUC, 0.03 STD\n",
    "0.053 Train-Val AUC Drift\n",
    "970.8 AVG Rounds, 505.52997932862496 STD\n",
    "\n",
    "--\n",
    "\n",
    "# V232\n",
    "0.874 AVG AUC, 0.003 STD\n",
    "0.945 AVG Train AUC, 0.01 STD\n",
    "0.071 Train-Val AUC Drift\n",
    "1801.0 AVG Rounds, 399.8169581195875 STD\n",
    "\n",
    "--\n",
    "# added all C columns\n",
    "0.909 AVG AUC, 0.005 STD\n",
    "0.969 AVG Train AUC, 0.009 STD\n",
    "0.06 Train-Val AUC Drift\n",
    "2011.2 AVG Rounds, 545.0619781272585 STD\n",
    "---\n",
    "0.907 AVG AUC, 0.005 STD\n",
    "0.966 AVG Train AUC, 0.008 STD\n",
    "0.058 Train-Val AUC Drift\n",
    "1759.8 AVG Rounds, 417.18648108489805 STD\n",
    "--\n",
    "0.907 AVG AUC, 0.004 STD\n",
    "0.969 AVG Train AUC, 0.003 STD\n",
    "0.061 Train-Val AUC Drift\n",
    "1886.2 AVG Rounds, 212.3387859059197 STD\n",
    "\n",
    "todo something more with c columns whats up with C1,6,13??? score very high importance\n",
    "FE on D columns, knowing what we know, how do we capture the streaks of frauds?\n",
    "V columns..\n",
    "period aggreations\n",
    "submission pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Folds Used\n",
      "0.2 Neg DownSample Frac with 1774 Seed\n",
      "0.907 AVG AUC, 0.004 STD\n",
      "0.969 AVG Train AUC, 0.003 STD\n",
      "0.061 Train-Val AUC Drift\n",
      "1886.2 AVG Rounds, 212.3387859059197 STD\n",
      "\n",
      "{'objective': 'binary', 'boosting_type': 'gbdt', 'metric': 'auc', 'n_jobs': -1, 'learning_rate': 0.01, 'num_leaves': 32, 'max_depth': -1, 'tree_learner': 'serial', 'colsample_bytree': 0.7, 'subsample': 0.7, 'subsample_freq': 1, 'max_bin': 255, 'verbose': -1, 'seed': 257, 'feature_fraction_seed': 259, 'bagging_seed': 260, 'drop_seed': 261, 'data_random_seed': 262}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/authman/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1706: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>adversarial</th>\n",
       "      <th>perm_import</th>\n",
       "      <th>perm_import_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>C13</td>\n",
       "      <td>0.024394</td>\n",
       "      <td>0.042799</td>\n",
       "      <td>0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>C1</td>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.033817</td>\n",
       "      <td>0.001174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>me2__p_and_r_email_grp__D3_mean</td>\n",
       "      <td>0.039590</td>\n",
       "      <td>0.022903</td>\n",
       "      <td>0.001935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>card2_grp__dist_mean</td>\n",
       "      <td>0.046235</td>\n",
       "      <td>0.022378</td>\n",
       "      <td>0.004064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>me2__card3_grp__TransactionAmt_mean</td>\n",
       "      <td>0.036731</td>\n",
       "      <td>0.016858</td>\n",
       "      <td>0.001940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>C6</td>\n",
       "      <td>0.013409</td>\n",
       "      <td>0.014420</td>\n",
       "      <td>0.003366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>C14</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.013373</td>\n",
       "      <td>0.002787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>C9</td>\n",
       "      <td>0.018573</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C2</td>\n",
       "      <td>0.017806</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>0.001576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>C5</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>addr1</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.001089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>p_and_r_email_grp__D3_mean</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.001145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dist</td>\n",
       "      <td>0.030105</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>me2__addr1_grp__TransactionAmtCents_mean</td>\n",
       "      <td>0.028226</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>0.001362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pemail_domain</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>addr1_grp__TransactionAmtCents_mean</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>0.001082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D8</td>\n",
       "      <td>0.012743</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M5</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.001198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>C8</td>\n",
       "      <td>0.017426</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.001262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>V232</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.000635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M6</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.000805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>p_and_r_email</td>\n",
       "      <td>0.025906</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.001439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>card34_grp__TransactionAmt_mean</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>me2__card34_grp__TransactionAmtCents_mean</td>\n",
       "      <td>0.030456</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>remail_domain_grp__D2_mean</td>\n",
       "      <td>0.016053</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionAmtCents</td>\n",
       "      <td>0.026553</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>remail_ext</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D9</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProductCD</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      feature  adversarial  perm_import  \\\n",
       "28                                        C13     0.024394     0.042799   \n",
       "22                                         C1     0.019059     0.033817   \n",
       "18            me2__p_and_r_email_grp__D3_mean     0.039590     0.022903   \n",
       "17                       card2_grp__dist_mean     0.046235     0.022378   \n",
       "7         me2__card3_grp__TransactionAmt_mean     0.036731     0.016858   \n",
       "25                                         C6     0.013409     0.014420   \n",
       "29                                        C14     0.013961     0.013373   \n",
       "27                                         C9     0.018573     0.007949   \n",
       "23                                         C2     0.017806     0.007646   \n",
       "24                                         C5     0.011284     0.004498   \n",
       "3                                       addr1     0.013235     0.004400   \n",
       "19                 p_and_r_email_grp__D3_mean     0.027596     0.004053   \n",
       "2                                        dist     0.030105     0.004006   \n",
       "9    me2__addr1_grp__TransactionAmtCents_mean     0.028226     0.003778   \n",
       "10                              pemail_domain     0.019043     0.003680   \n",
       "6         addr1_grp__TransactionAmtCents_mean     0.018700     0.003514   \n",
       "4                                          D8     0.012743     0.003413   \n",
       "14                                         M5     0.008772     0.002831   \n",
       "26                                         C8     0.017426     0.002636   \n",
       "13                                         M4     0.011672     0.002423   \n",
       "21                                       V232     0.013839     0.002320   \n",
       "15                                         M6     0.007605     0.002252   \n",
       "12                              p_and_r_email     0.025906     0.001892   \n",
       "16            card34_grp__TransactionAmt_mean     0.018067     0.001595   \n",
       "8   me2__card34_grp__TransactionAmtCents_mean     0.030456     0.001330   \n",
       "20                 remail_domain_grp__D2_mean     0.016053     0.000803   \n",
       "0                         TransactionAmtCents     0.026553     0.000556   \n",
       "11                                 remail_ext     0.010709     0.000149   \n",
       "5                                          D9     0.005534     0.000061   \n",
       "1                                   ProductCD     0.009348    -0.000032   \n",
       "\n",
       "    perm_import_std  \n",
       "28         0.004024  \n",
       "22         0.001174  \n",
       "18         0.001935  \n",
       "17         0.004064  \n",
       "7          0.001940  \n",
       "25         0.003366  \n",
       "29         0.002787  \n",
       "27         0.001397  \n",
       "23         0.001576  \n",
       "24         0.000771  \n",
       "3          0.001089  \n",
       "19         0.001145  \n",
       "2          0.001288  \n",
       "9          0.001362  \n",
       "10         0.001141  \n",
       "6          0.001082  \n",
       "4          0.000996  \n",
       "14         0.001198  \n",
       "26         0.000355  \n",
       "13         0.001262  \n",
       "21         0.000635  \n",
       "15         0.000805  \n",
       "12         0.001439  \n",
       "16         0.000303  \n",
       "8          0.000813  \n",
       "20         0.000507  \n",
       "0          0.000192  \n",
       "11         0.000256  \n",
       "5          0.000363  \n",
       "1          0.000114  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5af56a4630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report, sns_features = display_report(results)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(x=\"perm_import\", y=\"feature\", data=sns_features, edgecolor=('white'), linewidth=2)#, palette=\"rocket\")\n",
    "plt.title('Permutation Importance (averaged/folds)', fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to look at regular gain or splits, the V columns would dominate the show. But here we accurately see they really aren't that important in the grand scheme of things. Notice how some of our variables have a very high CVS score, especially the high cardinality categorical variables, like `card2` for example. While the unique values present for card2 in train and test are very similar, their distributions shift a bit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've decided on a subset of good engineered features to use, we must run the submission pipeline to submit. The submission pipeline is completely different from the feature validation pipeline. Rather, the sub pipeline builds many versions of a single model and merges them together. I wouldn't call this ensembling per se, since that'll happen is another, external script. But this is how we prepare our 'level-1' models for posting against Kaggle LB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VersionA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train on first 50% of Train data\n",
    "- Validate on second 50% of Train data\n",
    "- Repeat above two steps 3x, with different random _model_ seeds\n",
    "- Calculate `mean best rounds`\n",
    "- Ideally, `STD / Mean` rounds should be relatively 'low'\n",
    "- Train on second 50% of Train data using the `mean best rounds` calculated above without early stopping (no validation)\n",
    "- Repeat above step 3x, with different random _model_ seeds\n",
    "- Train on entire (100%) Train data using 2x rounds `mean best rounds` calculated above without early stopping (no validation)\n",
    "- So we have:\n",
    "    - A) 3 models (different seeds) trained on first 50% of train data, each with unique `best rounds`.\n",
    "    - B) 3 models (different seeds) trained on second 50% of train data, each with the same `mean best rounds`\n",
    "    - C) 3 models (different seeds) trained on 100% of train data, each with the same `mean best rounds * 2`\n",
    "- Use each of these models to predict the submission set:\n",
    "    - Take the mean val of A), the mean val of B), and the mean val of C)\n",
    "- Try two merging strategies for final LB submission:\n",
    "    - A^0.2 + B^0.2 + C^0.6, and validate LB\n",
    "    - (A + B + C) / 3, and validate LB\n",
    "- Use whichever of the last two strategies works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VersionB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train on first 33% of Train data\n",
    "- Validate on second 33% of Train data\n",
    "- Repeat above two steps 3x, with different random _model_ seeds\n",
    "- Train on second 33% of Train data\n",
    "- Validate on third 33% of Train data\n",
    "- Repeat above two steps 3x, with different random _model_ seeds\n",
    "- Calculate `mean best rounds`\n",
    "- Ideally, `STD / Mean` rounds should be relatively 'low'\n",
    "- Train on third 33% of Train data using the `mean best rounds` calculated above without early stopping (no validation)\n",
    "- Repeat above step 3x, with different random _model_ seeds\n",
    "- Train on entire (100%) Train data using 3x rounds `mean best rounds` calculated above without early stopping (no validation)\n",
    "- So we have:\n",
    "    - A) 3 models (different seeds) trained on first 33% of train data, each with unique `best rounds`.\n",
    "    - C) 3 models (different seeds) trained on second 33% of train data, each with unique `best rounds`.\n",
    "    - B) 3 models (different seeds) trained on third 33% of train data, each with the same `mean best rounds`\n",
    "    - C) 3 models (different seeds) trained on 100% of train data, each with the same `mean best rounds * 3`\n",
    "- Use each of these models to predict the submission set:\n",
    "    - Take the mean val of A), the mean val of B), the mean val of C), and the mean val of D)\n",
    "- Try two merging strategies for final LB submission:\n",
    "    - A^0.166 + B^0.166 + C^0.166 + D^0.6, and validate LB\n",
    "    - (A + B + C + D) / 4, and validate LB\n",
    "- Use whichever of the last two strategies works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VersionC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train on first 75% of Train data\n",
    "- Validate on second 25% of Train data\n",
    "- Repeat above two steps 3x, with different random _model_ seeds\n",
    "- Calculate `mean best rounds`\n",
    "- Ideally, `STD / Mean` rounds should be relatively 'low'\n",
    "- Train on remaining 25% of Train data using the `mean best rounds / 3` calculated above without early stopping (no validation)\n",
    "- Repeat above step 3x, with different random _model_ seeds\n",
    "- Train on entire (100%) Train data using 1.333x rounds `mean best rounds` calculated above without early stopping (no validation)\n",
    "- So we have:\n",
    "    - A) 3 models (different seeds) trained on first 75% of train data, each with unique `best rounds`.\n",
    "    - B) 3 models (different seeds) trained on second 25% of train data, each with the same `mean best rounds / 3`\n",
    "    - C) 3 models (different seeds) trained on 100% of train data, each with the same `mean best rounds * 1.333`\n",
    "- Use each of these models to predict the submission set:\n",
    "    - Take the mean val of A), the mean val of B), and the mean val of C)\n",
    "- Try two merging strategies for final LB submission:\n",
    "    - A^0.2 + B^0.2 + C^0.6, and validate LB\n",
    "    - (A + B + C) / 3, and validate LB\n",
    "- Use whichever of the last two strategies works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES**\n",
    "\n",
    "1. Ideally, the best rounds for the 100% training across these versions should be the sameish.\n",
    "1. Once we know if geometric or arithmetic mean works best, repeat the above process 5x times, each time using a different 20% (no replacement) non-fraud negative downsampling and straight average results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_seeds(params, new_seed=1337):\n",
    "    params['seed'] = new_seed\n",
    "    params['feature_fraction_seed'] = new_seed + 3\n",
    "    params['bagging_seed'] = new_seed + 5\n",
    "    params['drop_seed'] = new_seed + 7\n",
    "    params['data_random_seed'] = new_seed + 11\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data, features, params, mask_train=None, mask_val=None, num_rounds=None, cat_features=None):\n",
    "    if mask_train is None:\n",
    "        trn = data[mask_train]\n",
    "    else:\n",
    "        trn = data\n",
    "    trn_lgb = lgb.Dataset(trn[features], label=trn.isFraud)\n",
    "    \n",
    "    if mask_val is not None:\n",
    "        val_lgb = lgb.Dataset(val[features], label=val.isFraud)\n",
    "        val = data[mask_val]\n",
    "        clf = lgb.train(\n",
    "            params,\n",
    "            trn_lgb,\n",
    "            valid_sets = [trn_lgb, val_lgb],\n",
    "            verbose_eval = 200,\n",
    "            early_stopping_rounds = 33,\n",
    "            num_boost_round = 80000,\n",
    "            categorical_feature = cat_features\n",
    "        )\n",
    "        num_rounds = clf.best_score['valid_1']['auc']\n",
    "        \n",
    "    else:\n",
    "        clf = lgb.train(\n",
    "            params,\n",
    "            trn_lgb,\n",
    "            verbose_eval = 200,\n",
    "            num_boost_round = num_rounds,\n",
    "            categorical_feature = cat_features\n",
    "        )\n",
    "        num_rounds = None\n",
    "        \n",
    "    return clf, num_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_base_submission(data, sub_data, features, params, version='a', runs=1, seed=1337, cat_features=None):\n",
    "    # Change runs to 3 for a proper (final) sub\n",
    "    # Runs can be 1 for a 'faster' sub\n",
    "    version = version.lower()\n",
    "    params = params.copy() # dont mess up the original\n",
    "    train = data[~data.isFraud.isna()]\n",
    "    print('Training Version:', version, 'with', runs, 'runs.')\n",
    "    \n",
    "    folder = './' + str(datetime.datetime.now()).replace('-','').replace(':','').split('.')[0].replace(' ','_') + '_v'+version\n",
    "    !mkdir folder\n",
    "    \n",
    "    # Save our info\n",
    "    with open(folder + '/info.txt', 'a') as handle:\n",
    "        handle.write('Training Version:', version, 'with', runs, 'runs.')\n",
    "        handle.write('Params:\\n' + json.dumps(params, ensure_ascii=False, indent=4))\n",
    "        handle.write('\\n\\nFeatures: ' + ', '.join(features))\n",
    "    \n",
    "    if version=='a':\n",
    "        ########################################################################\n",
    "        # Train on first 50% of Train data\n",
    "        # Validate on second 50% of Train data\n",
    "        # Repeat above two steps 3x, with different random model seeds\n",
    "        splitPoint = (train.TransactionDT.max() - train.TransactionDT.min()) / 2\n",
    "        splitA = train.TransactionDT<=splitPoint\n",
    "        splitB = train.TransactionDT> splitPoint\n",
    "        \n",
    "        roundsA = []\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                mask_train = splitA,\n",
    "                mask_val = splitB,\n",
    "                num_rounds = None, # let it compute\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "            roundsA.append(num_rounds)\n",
    "            with open(folder + '/info.txt', 'a') as handle:\n",
    "                handle.write('Run {}: {} Num Rounds: {}, {} AUC\\n'.format(run+1, num_rounds, clf.best_score['valid_1']['auc']))\n",
    "                \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splita_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splita_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Calculate mean best rounds\n",
    "        # Ideally, STD / Mean rounds should be relatively 'low'\n",
    "        roundsA = np.array(roundsA)\n",
    "        mean_roundsA = int(roundsA.mean())\n",
    "        std_roundsA = roundsA.std()\n",
    "        print('{:.0f} mean best rounds, with {:.3f} STD'.format(mean_roundsA, std_roundsA))\n",
    "        if std_roundsA/mean_roundsA > 0.333: print('NOTE: std/mean is pretty high here...')\n",
    "            \n",
    "        # Train on second 50% of Train data using the mean best rounds calculated above without early stopping (no validation)\n",
    "        # Repeat above step 3x, with different random model seeds\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                mask_train = splitB,\n",
    "                num_rounds = mean_roundsA,\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "        \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splitb_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splitb_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "        # Train on entire (100%) Train data using 2x rounds mean best rounds calculated above without early stopping (no validation)\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                num_rounds = mean_roundsA * 2,\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "        \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splitc_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splitc_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "            \n",
    "        # So we have:\n",
    "        # A) `runs` models (different seeds) trained on first 50% of train data, each with unique best rounds.\n",
    "        # B) `runs` models (different seeds) trained on second 50% of train data, each with the same mean best rounds\n",
    "        # C) `runs` models (different seeds) trained on 100% of train data, each with the same mean best rounds * 2\n",
    "        # Use each of these models to predict the submission set (already done)\n",
    "        \n",
    "        # Take the mean val of A), the mean val of B), and the mean val of C)\n",
    "        for split in 'abc':\n",
    "            preds = np.concatenate([\n",
    "                pd.read_csv(folder + '/split{}_run{}.csv'.format(split,run+1)).isFraud.values.reshape(-1,1)\n",
    "                for run in range(runs)\n",
    "            ], axis=1).mean(axis=1)\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': preds.flatten()\n",
    "            }).to_csv(folder + '/split{}_mean.csv'.format(split), index=False)\n",
    "        \n",
    "\n",
    "        # Try two merging strategies for final LB submission:\n",
    "        # Use whichever of the last two strategies works best.\n",
    "        splits = [\n",
    "            pd.read_csv(folder + '/split{}_mean.csv'.format(split)).isFraud.values.reshape(-1,1)\n",
    "            for split in 'abc'\n",
    "        ]\n",
    "        pd.DataFrame({\n",
    "            'TransactionID': sub_data.TransactionID,\n",
    "            'isFraud': (splits[0]**0.2 + splits[1]**0.2 + splits[2]**0.6).flatten()\n",
    "        }).to_csv(folder + '/sub_geometric_mean.csv', index=False)\n",
    "        pd.DataFrame({\n",
    "            'TransactionID': sub_data.TransactionID,\n",
    "            'isFraud': (splits[0] + splits[1] + splits[2]).flatten() # No need to even /3 since AUC\n",
    "        }).to_csv(folder + '/sub_arithmetic_mean.csv', index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    elif version=='b':\n",
    "        ########################################################################\n",
    "        # Train on first 33% of Train data\n",
    "        # Validate on second 33% of Train data\n",
    "        # Repeat above two steps 3x, with different random model seeds\n",
    "        splitPoint1 = (train.TransactionDT.max() - train.TransactionDT.min()) / 3\n",
    "        splitPoint2 = (train.TransactionDT.max() - train.TransactionDT.min()) / 3 * 2\n",
    "        splitA = train.TransactionDT<=splitPoint1\n",
    "        splitB = (train.TransactionDT>splitPoint1) & (train.TransactionDT<=splitPoint2)\n",
    "        splitC = train.TransactionDT>splitPoint2\n",
    "        \n",
    "        roundsA = []\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                mask_train = splitA,\n",
    "                mask_val = splitB,\n",
    "                num_rounds = None, # let it compute\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "            roundsA.append(num_rounds)\n",
    "            with open(folder + '/info.txt', 'a') as handle:\n",
    "                handle.write('RunA {}: {} Num Rounds: {}, {} AUC\\n'.format(run+1, num_rounds, clf.best_score['valid_1']['auc']))\n",
    "                \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splita_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splita_run{}.csv'.format(run+1), index=False)\n",
    "          \n",
    "        \n",
    "        # Train on second 33% of Train data\n",
    "        # Validate on third 33% of Train data\n",
    "        # Repeat above two steps 3x, with different random model seeds\n",
    "        roundsB = []\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                mask_train = splitB,\n",
    "                mask_val = splitC,\n",
    "                num_rounds = None, # let it compute\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "            roundsB.append(num_rounds)\n",
    "            with open(folder + '/info.txt', 'a') as handle:\n",
    "                handle.write('RunB {}: {} Num Rounds: {}, {} AUC\\n'.format(run+1, num_rounds, clf.best_score['valid_1']['auc']))\n",
    "                \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splitb_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splitb_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Calculate mean best rounds\n",
    "        # Ideally, STD / Mean rounds should be relatively 'low'\n",
    "        roundsA = np.array(roundsA)\n",
    "        roundsB = np.array(roundsB)\n",
    "        mean_roundsA = int(roundsA.mean())\n",
    "        mean_roundsB = int(roundsB.mean())\n",
    "        std_roundsA = roundsA.std()\n",
    "        std_roundsB = roundsB.std()\n",
    "        print('A) {:.0f} mean best rounds, with {:.3f} STD'.format(mean_roundsA, std_roundsA))\n",
    "        print('B) {:.0f} mean best rounds, with {:.3f} STD'.format(mean_roundsB, std_roundsB))\n",
    "        rounds = np.array(roundsA + roundsB)\n",
    "        mean_rounds = int(rounds.mean())\n",
    "        std_rounds = rounds.std()\n",
    "        if std_roundsA/mean_roundsA > 0.333: print('NOTE: A) std/mean is pretty high here...')\n",
    "        if std_roundsB/mean_roundsB > 0.333: print('NOTE: B) std/mean is pretty high here...')\n",
    "        if std_rounds/mean_rounds > 0.333: print('NOTE: *) std/mean is pretty high here...')\n",
    "        \n",
    "        \n",
    "        # Train on third 33% of Train data using the mean best rounds calculated above without early stopping (no validation)\n",
    "        # Repeat above step 3x, with different random model seeds\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                mask_train = splitC,\n",
    "                num_rounds = mean_rounds,\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "        \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splitc_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splitc_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "        # Train on entire (100%) Train data using 3x rounds mean best rounds calculated above without early stopping (no validation)\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                num_rounds = mean_rounds * 3,\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "        \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splitd_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splitd_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "            \n",
    "        # So we have:\n",
    "        # A) 3 models (different seeds) trained on first 33% of train data, each with unique best rounds.\n",
    "        # C) 3 models (different seeds) trained on second 33% of train data, each with unique best rounds.\n",
    "        # B) 3 models (different seeds) trained on third 33% of train data, each with the same mean best rounds\n",
    "        # C) 3 models (different seeds) trained on 100% of train data, each with the same mean best rounds * 3\n",
    "        # Use each of these models to predict the submission set (already done)\n",
    "        \n",
    "        # Take the mean val of A), the mean val of B), the mean val of C), and the mean val of D)\n",
    "        for split in 'abcd':\n",
    "            preds = np.concatenate([\n",
    "                pd.read_csv(folder + '/split{}_run{}.csv'.format(split,run+1)).isFraud.values.reshape(-1,1)\n",
    "                for run in range(runs)\n",
    "            ], axis=1).mean(axis=1)\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': preds.flatten()\n",
    "            }).to_csv(folder + '/split{}_mean.csv'.format(split), index=False)\n",
    "        \n",
    "\n",
    "        # Try two merging strategies for final LB submission:\n",
    "        # Use whichever of the last two strategies works best.\n",
    "        splits = [\n",
    "            pd.read_csv(folder + '/split{}_mean.csv'.format(split)).isFraud.values.reshape(-1,1)\n",
    "            for split in 'abcd'\n",
    "        ]\n",
    "        pd.DataFrame({\n",
    "            'TransactionID': sub_data.TransactionID,\n",
    "            'isFraud': (splits[0]**0.1666 + splits[1]**0.1666 + splits[2]**0.1666 + splits[2]**0.4).flatten()\n",
    "        }).to_csv(folder + '/sub_geometric_mean.csv', index=False)\n",
    "        pd.DataFrame({\n",
    "            'TransactionID': sub_data.TransactionID,\n",
    "            'isFraud': (splits[0] + splits[1] + splits[2] + split[3]).flatten() # No need to even /4 since AUC\n",
    "        }).to_csv(folder + '/sub_arithmetic_mean.csv', index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        ########################################################################\n",
    "        # Train on first 75% of Train data\n",
    "        # Validate on second 25% of Train data\n",
    "        # Repeat above two steps 3x, with different random model seeds\n",
    "        splitPoint = (train.TransactionDT.max() - train.TransactionDT.min()) / 4 * 3\n",
    "        splitA = train.TransactionDT<=splitPoint\n",
    "        splitB = train.TransactionDT> splitPoint\n",
    "        \n",
    "        roundsA = []\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                mask_train = splitA,\n",
    "                mask_val = splitB,\n",
    "                num_rounds = None, # let it compute\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "            roundsA.append(num_rounds)\n",
    "            with open(folder + '/info.txt', 'a') as handle:\n",
    "                handle.write('Run {}: {} Num Rounds: {}, {} AUC\\n'.format(run+1, num_rounds, clf.best_score['valid_1']['auc']))\n",
    "                \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splita_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splita_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Calculate mean best rounds\n",
    "        # Ideally, STD / Mean rounds should be relatively 'low'\n",
    "        roundsA = np.array(roundsA)\n",
    "        mean_roundsA = int(roundsA.mean())\n",
    "        std_roundsA = roundsA.std()\n",
    "        print('{:.0f} mean best rounds, with {:.3f} STD'.format(mean_roundsA, std_roundsA))\n",
    "        if std_roundsA/mean_roundsA > 0.333: print('NOTE: std/mean is pretty high here...')\n",
    "            \n",
    "        # Train on second 25% of Train data using the mean best rounds / 3 calculated above without early stopping (no validation)\n",
    "        # Repeat above step 3x, with different random model seeds\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                mask_train = splitB,\n",
    "                num_rounds = int(mean_roundsA/3),\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "        \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splitb_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splitb_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "        # Train on entire (100%) Train data using 2x rounds mean best rounds calculated above without early stopping (no validation)\n",
    "        for run in range(runs):\n",
    "            params = update_seeds(params, new_seed=seed+run)\n",
    "            clf, num_rounds = build_model(\n",
    "                train,\n",
    "                features,\n",
    "                params,\n",
    "                num_rounds = int(mean_roundsA * 1.3333),\n",
    "                cat_features = cat_features\n",
    "            )\n",
    "        \n",
    "            # Save the model\n",
    "            clf.save_model(folder + '/splitc_run{}.lgb'.format(run+1))\n",
    "            \n",
    "            # Save submission preds\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': clf.predict(sub_data[features]).flatten(),\n",
    "            }).to_csv(folder + '/splitc_run{}.csv'.format(run+1), index=False)\n",
    "            \n",
    "            \n",
    "        # So we have:\n",
    "        # A) 3 models (different seeds) trained on first 75% of train data, each with unique best rounds.\n",
    "        # B) 3 models (different seeds) trained on second 25% of train data, each with the same mean best rounds / 3\n",
    "        # C) 3 models (different seeds) trained on 100% of train data, each with the same mean best rounds * 1.3333\n",
    "        # Use each of these models to predict the submission set (already done)\n",
    "        \n",
    "        # Take the mean val of A), the mean val of B), and the mean val of C)\n",
    "        for split in 'abc':\n",
    "            preds = np.concatenate([\n",
    "                pd.read_csv(folder + '/split{}_run{}.csv'.format(split,run+1)).isFraud.values.reshape(-1,1)\n",
    "                for run in range(runs)\n",
    "            ], axis=1).mean(axis=1)\n",
    "            pd.DataFrame({\n",
    "                'TransactionID': sub_data.TransactionID,\n",
    "                'isFraud': preds.flatten()\n",
    "            }).to_csv(folder + '/split{}_mean.csv'.format(split), index=False)\n",
    "        \n",
    "\n",
    "        # Try two merging strategies for final LB submission:\n",
    "        # Use whichever of the last two strategies works best.\n",
    "        splits = [\n",
    "            pd.read_csv(folder + '/split{}_mean.csv'.format(split)).isFraud.values.reshape(-1,1)\n",
    "            for split in 'abc'\n",
    "        ]\n",
    "        pd.DataFrame({\n",
    "            'TransactionID': sub_data.TransactionID,\n",
    "            'isFraud': (splits[0]**0.2 + splits[1]**0.2 + splits[2]**0.6).flatten()\n",
    "        }).to_csv(folder + '/sub_geometric_mean.csv', index=False)\n",
    "        pd.DataFrame({\n",
    "            'TransactionID': sub_data.TransactionID,\n",
    "            'isFraud': (splits[0] + splits[1] + splits[2]).flatten() # No need to even /3 since AUC\n",
    "        }).to_csv(folder + '/sub_arithmetic_mean.csv', index=False)\n",
    "    \n",
    "    \n",
    "    return folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1-time only):\n",
    "# Run version a locally with runs=3\n",
    "# Check on Val and LB which is better, geomean or arimean\n",
    "# Comment out the code for the one that is worse.\n",
    "\n",
    "# TODO (1-time only())\n",
    "# Run version a, b, and c locally with runs=3\n",
    "# Compare a) numrounds*2, b) numrounds*3, c) numrounds*1.333\n",
    "# Hopefully the values aren't too different\n",
    "# Check on Val and LB of each version, which is better\n",
    "# Only use that version going forward\n",
    "\n",
    "# TODO (each LB submission)\n",
    "# Use np.random.choice w replace=False to chop up train[traintr.isFraud==0] into 20% chunks\n",
    "# For each 20% chunk of isFraud=0\n",
    "# - Merge the 20% isFraud=0 with 100% isFraud=1\n",
    "# - Run the best 'version' locally with runs=3 and whatever is better geo-mean, or arith-mean\n",
    "# - Check on Val values of each chunk\n",
    "# Take the mean of all 5 chunk predictions\n",
    "# Submit for LB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = prepare_base_submission(\n",
    "#     data,\n",
    "#     sub_data,\n",
    "#     features,\n",
    "#     params,\n",
    "#     version='a',\n",
    "#     runs=1,\n",
    "#     seed=1337,\n",
    "#     cat_features=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Hereafter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ProductCD\n",
    "\n",
    "C     68519, 8008 frauds and transactions are all over, 443 unique vals\n",
    "H     33024, 1574 all frauds and trx have cents=0\n",
    "R     37699, 1426 all frauds and trx have cents=0\n",
    "S     11628, 686  almost all trx @0, some at non-zero\n",
    "W    439670, 8969, well heterogenius 3224 unique vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
